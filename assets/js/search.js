
var documents = [{
    "id": 0,
    "url": "https://nipunbatra.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://nipunbatra.github.io/categories/",
    "title": "Tags",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; • {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 2,
    "url": "https://nipunbatra.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 3,
    "url": "https://nipunbatra.github.io/ml/2020/02/20/bayesian-linear-regression.html",
    "title": "Bayesian Linear Regression",
    "body": "2020/02/20 -                 import numpy as npimport pandas as pdimport matplotlib. pyplot as plt%matplotlib inline          x = np. linspace(-1, 1, 50). reshape(-1, 1)          y = 5*x + 4 noise = (np. abs(x. flatten())*np. random. randn(len(x))). reshape(-1,1)y = y + noise          plt. scatter(x, y)plt. plot(x, 5*x + 4, &#39;k&#39;)  [&lt;matplotlib. lines. Line2D at 0x115c28cd0&gt;]        from scipy. stats import multivariate_normalfrom matplotlib import cmcov = np. array([[ 1 , 0], [0, 1]])var = multivariate_normal(mean=[0,0], cov=cov)x_grid, y_grid = np. mgrid[-1:1:. 01, -1:1:. 01]pos = np. dstack((x_grid, y_grid))z = var. pdf(pos)plt. contourf(x_grid, y_grid, z)plt. gca(). set_aspect(&#39;equal&#39;)plt. xlabel(r&quot;$\theta_0$&quot;)plt. ylabel(r&quot;$\theta_1$&quot;)plt. title(r&quot;Prior distribution of $\theta = f(\mu, \Sigma)$&quot;)plt. colorbar()  &lt;matplotlib. colorbar. Colorbar at 0x1a18423950&gt;  $$\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(y_{i}-\hat{y}_{i})^{2}}{2 \sigma^{2}}}$$Sample from prior&#182;:       n_samples = 20for n in range(n_samples):  theta_0_s, theta_1_s = var. rvs()  plt. plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0. 2)plt. scatter(x, y)  &lt;matplotlib. collections. PathCollection at 0x1a18598fd0&gt;  Likelihood of theta&#182;:       def likelihood(theta_0, theta_1, x, y, sigma):  s = 0  x_plus_1 = np. hstack((np. ones_like(x), x))  for i in range(len(x)):    y_i_hat = x_plus_1[i, :]@np. array([theta_0, theta_1])    s += (y[i,:]-y_i_hat)**2      return np. exp(-s/(2*sigma*sigma))/np. sqrt(2*np. pi*sigma*sigma)          likelihood(-1, 1, x, y, 4)  array([1. 00683395e-22])        x_grid_2, y_grid_2 = np. mgrid[0:8:. 1, 0:8:. 1]li = np. zeros_like(x_grid_2)for i in range(x_grid_2. shape[0]):  for j in range(x_grid_2. shape[1]):    li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j], x, y, 4)              plt. contourf(x_grid_2, y_grid_2, li)plt. gca(). set_aspect(&#39;equal&#39;)plt. xlabel(r&quot;$\theta_0$&quot;)plt. ylabel(r&quot;$\theta_1$&quot;)plt. colorbar()plt. scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;)plt. title(r&quot;Likelihood as a function of ($\theta_0, \theta_1$)&quot;)  Text(0. 5, 1. 0, &#39;Likelihood as a function of ($\\theta_0, \\theta_1$)&#39;)  Likelihood of $\sigma^2$&#182;:       x_plus_1 = np. hstack((np. ones_like(x), x))theta_mle = np. linalg. inv(x_plus_1. T@x_plus_1)@(x_plus_1. T@y)sigma_2_mle = np. linalg. norm(y - x_plus_1@theta_mle)**2sigma_mle = np. sqrt(sigma_2_mle)sigma_mle  4. 128685902124939  Posterior&#182;: $$\begin{aligned}p(\boldsymbol{\theta} | \mathcal{X}, \mathcal{Y}) &amp;=\mathcal{N}\left(\boldsymbol{\theta} | \boldsymbol{m}_{N}, \boldsymbol{S}_{N}\right) \\\boldsymbol{S}_{N} &amp;=\left(\boldsymbol{S}_{0}^{-1}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi}\right)^{-1} \\\boldsymbol{m}_{N} &amp;=\boldsymbol{S}_{N}\left(\boldsymbol{S}_{0}^{-1} \boldsymbol{m}_{0}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{y}\right)\end{aligned}$$      S0 = np. array([[ 1 , 0], [0, 1]])M0 = np. array([0, 0])SN = np. linalg. inv(np. linalg. inv(S0) + (sigma_mle**-2)*x_plus_1. T@x_plus_1)MN = SN@(np. linalg. inv(S0)@M0 + (sigma_mle**-2)*(x_plus_1. T@y). squeeze())          MN, SN  (array([2. 97803341, 2. 54277597]), array([[2. 54243881e-01, 2. 97285330e-17],    [2. 97285330e-17, 4. 95625685e-01]]))        from scipy. stats import multivariate_normalfrom matplotlib import cmcov = np. array([[ 1 , 0], [0, 1]])var_pos = multivariate_normal(mean=MN, cov=SN)x_grid, y_grid = np. mgrid[0:8:. 1, 0:8:. 1]pos = np. dstack((x_grid, y_grid))z = var_pos. pdf(pos)plt. contourf(x_grid, y_grid, z)plt. gca(). set_aspect(&#39;equal&#39;)plt. xlabel(r&quot;$\theta_0$&quot;)plt. ylabel(r&quot;$\theta_1$&quot;)plt. title(r&quot;Posterior distribution of $\theta = f(\mu, \Sigma)$&quot;)plt. scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;, label=&#39;MLE&#39;)plt. scatter(MN[0], MN[1], s=100, marker=&#39;^&#39;, color=&#39;black&#39;, label=&#39;MAP&#39;)plt. colorbar()plt. legend()plt. savefig(&quot;. . /images/blr-map. png&quot;)    Sample from posterior       n_samples = 20for n in range(n_samples):  theta_0_s, theta_1_s = var_pos. rvs()  plt. plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0. 2)plt. scatter(x, y)  &lt;matplotlib. collections. PathCollection at 0x1a18e7dd10&gt;  Posterior predictions&#182;: $$\begin{aligned}p\left(y_{*} | \mathcal{X}, \mathcal{Y}, \boldsymbol{x}_{*}\right) &amp;=\int p\left(y_{*} | \boldsymbol{x}_{*}, \boldsymbol{\theta}\right) p(\boldsymbol{\theta} | \mathcal{X}, \mathcal{Y}) \mathrm{d} \boldsymbol{\theta} \\&amp;=\int \mathcal{N}\left(y_{*} | \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{\theta}, \sigma^{2}\right) \mathcal{N}\left(\boldsymbol{\theta} | \boldsymbol{m}_{N}, \boldsymbol{S}_{N}\right) \mathrm{d} \boldsymbol{\theta} \\&amp;=\mathcal{N}\left(y_{*} | \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{m}_{N}, \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{S}_{N} \boldsymbol{\phi}\left(\boldsymbol{x}_{*}\right)+\sigma^{2}\right)\end{aligned}$$For a point $x*$ Predictive mean = $X^Tm_N$ Predictive variance = $X^TS_NX + \sigma^2$       x_plus_1. T. shape, SN. shape, x_plus_1. shape  ((2, 50), (2, 2), (50, 2))        pred_var = x_plus_1@SN@x_plus_1. Tpred_var. shape  (50, 50)        ## Marginalindividual_var = pred_var. diagonal()          y_hat_map = x_plus_1@MNplt. plot(x, y_hat_map, color=&#39;black&#39;)plt. fill_between(x. flatten(), y_hat_map-individual_var, y_hat_map+individual_var, alpha=0. 2, color=&#39;black&#39;)plt. scatter(x, y)  &lt;matplotlib. collections. PathCollection at 0x1a1881e450&gt;  "
    }, {
    "id": 4,
    "url": "https://nipunbatra.github.io/markdown/2020/01/14/test-markdown.html",
    "title": "Example Markdown Post",
    "body": "2020/01/14 - Basic setup: Jekyll requires blog post files to be named according to the following format: YEAR-MONTH-DAY-filename. md Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. . md is the file extension for markdown files. The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. Basic formatting: You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: Lists: Here’s a list:  item 1 item 2And a numbered list:  item 1 item 2Boxes and stuff:  This is a quotation    You can include alert boxes…and…    You can include info boxesImages: Code: General preformatted text: # Do a thingdo_thing()Python code and output: # Prints '2'print(1+1)2Tables:       Column 1   Column 2         A thing   Another thing   Tweetcards: Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019Footnotes:       This is the footnote.  &#8617;    "
    }, {
    "id": 5,
    "url": "https://nipunbatra.github.io/ml/2019/08/20/Gaussian-Processes.html",
    "title": "Gaussian Processes",
    "body": "2019/08/20 -           An example&#182;: Let us look at the GIF above. It shows a non-linear fit with uncertainty on a set of points in the 2d space. The uncertainty is shown by the gray shadowed region. The animation shows how the fit and the uncertainty varies as we keep adding more points (shown as big circles). As expected, as more points are added, the uncertainty of the fit in the vicinity of the added points reduces. This is an example of Gaussian Processes (GP) regression in play. Introduction&#182;: There exist some great online resources for Gaussian Processes (GPs) including an excellent recent Distill. Pub article. This blog post is an attempt with a programatic flavour. In this notebook, we will build the intuition and learn some basics of GPs. This notebook is heavily inspired by the awesome tutorial by Richard Turner. Here is the link to the slides and video. Lectures videos and notes from Nando De Freitas' course are an amazing resource for GPs (and anything ML!). Some imports&#182;:       import numpy as npimport matplotlib. pyplot as pltimport warningswarnings. filterwarnings(&#39;ignore&#39;)%matplotlib inline    A function to make the Matplotlib plots prettier&#182;:       SPINE_COLOR = &#39;gray&#39;def format_axes(ax):  for spine in [&#39;top&#39;, &#39;right&#39;]:    ax. spines[spine]. set_visible(False)  for spine in [&#39;left&#39;, &#39;bottom&#39;]:    ax. spines[spine]. set_color(SPINE_COLOR)    ax. spines[spine]. set_linewidth(0. 5)  ax. xaxis. set_ticks_position(&#39;bottom&#39;)  ax. yaxis. set_ticks_position(&#39;left&#39;)  for axis in [ax. xaxis, ax. yaxis]:    axis. set_tick_params(direction=&#39;out&#39;, color=SPINE_COLOR)  return ax    One dimensional Gaussian/Normal&#182;: We will start the discussion with 1d Gaussians. Let us write some simple code to generate/sample data from $\mathcal{N}(\mu=0, \sigma=1)$       one_dim_normal_data = np. random. normal(0, 1, size=10000)    Let us now visualise the data in a 1d space using scatter plot       plt. scatter(one_dim_normal_data, np. zeros_like(one_dim_normal_data), alpha=0. 2, c=&#39;gray&#39;, edgecolors=&#39;k&#39;, marker=&#39;o&#39;)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2f0be5b00&gt;  As we would expect, there are a lot of samples close to zero (mean) and as we go further away from zero, the number of samples keeps reducing. We can also visualise the same phenomenon using a normed histogram shown below.       plt. hist(one_dim_normal_data, density=True, bins=20, color=&#39;gray&#39;)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2f0813588&gt;  We can notice that there is a high probability of drawing samples close to the mean and the probability is low far from the mean. However, since histograms come with their own set of caveats, let us use kernel desnity estimation for obtaining the probability density of 1d Gaussian.       from sklearn. neighbors import KernelDensityx_d = np. linspace(-4, 4, 100)# instantiate and fit the KDE modelkde = KernelDensity(bandwidth=1. 0, kernel=&#39;gaussian&#39;)kde. fit(one_dim_normal_data[:, None])# score_samples returns the log of the probability densitylogprob = kde. score_samples(x_d[:, None])plt. fill_between(x_d, np. exp(logprob), alpha=0. 2, color=&#39;gray&#39;)plt. plot(one_dim_normal_data, np. full_like(one_dim_normal_data, -0. 01), &#39;|k&#39;, markeredgewidth=0. 1)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2ee1f8c88&gt;  We can now see a smoother version of the histogram and can again verify the properties of 1D Gaussian. Let us now vary the variance of 1D Gaussian and make the same plots to enhance our understanding of the concept.       fig, ax = plt. subplots(ncols=3, sharey=True, figsize=(9, 3))x_d = np. linspace(-6, 6, 400)for i, var in enumerate([0. 5, 1, 2]):  one_dim_normal_data = np. random. normal(0, var, size=10000)  kde = KernelDensity(bandwidth=1. 0, kernel=&#39;gaussian&#39;)  kde. fit(one_dim_normal_data[:, None])  # score_samples returns the log of the probability density  logprob = kde. score_samples(x_d[:, None])  ax[i]. fill_between(x_d, np. exp(logprob), alpha=0. 2, color=&#39;gray&#39;)  ax[i]. plot(one_dim_normal_data, np. full_like(one_dim_normal_data, -0. 01), &#39;|k&#39;, markeredgewidth=0. 1)  format_axes(ax[i])  ax[i]. set_title(f&quot;Variance = {var}&quot;)    We can see that how increasing the variance makes the data more spread. Bi-variate Gaussian&#182;: Having discussed the case of 1d Gaussian, now let us move to multivariate Gaussians. As a special case, let us first consider bi-variate or 2d Gaussian. It's parameters are the mean vector which will have 2 elements and a covariance matrix. We can write the distribution as:$$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mu_1 \\ \mu_2\end{pmatrix} , \begin{pmatrix} a &amp; \rho \\ \rho &amp; b\end{pmatrix} \right)$$ where $\mu_1$, $\mu_2$ are the means for $X_1$ and $X_2$ respectively; $a$ is the standard deviation for $X_1$, $b$ is the standard deviation for $X_2$ and $\rho$ is the correlation between $X_1$ and $X_2$ Let us now draw some data from: $$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} 0 \\ 0\end{pmatrix} , \begin{pmatrix} 1 &amp; 0. 7 \\ 0. 7 &amp; 1\end{pmatrix} \right)$$       data = np. random. multivariate_normal(mean = np. array([0, 0]), cov = np. array([[1, 0. 7], [0. 7, 1]]), size=(10000, ))          plt. scatter(data[:, 0], data[:, 1], alpha=0. 05,c=&#39;gray&#39;)plt. axhline(0, color=&#39;k&#39;, lw=0. 2)plt. axvline(0, color=&#39;k&#39;, lw=0. 2)plt. xlabel(r&quot;$X_1$&quot;)plt. ylabel(r&quot;$X_2$&quot;)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2edfeb780&gt;  We can see from the plot above that the data is distributed around mean [0, 0]. We can also see the positive correlation between $X_1$ and $X_2$ Marginalisation for bivariate Gaussian&#182;: Let us look into an interesting plot provided by Seaborn.       import pandas as pddata_df = pd. DataFrame(data, columns=[r&#39;$X_1$&#39;,r&#39;$X_2$&#39;])          import seaborn as snsg = sns. jointplot(x= r&#39;$X_1$&#39;, y=r&#39;$X_2$&#39;, data=data_df, kind=&quot;reg&quot;,color=&#39;gray&#39;)    The central plot is exactly the same as the scatter plot we made earlier. But, we see two additional 1d KDE plots at the top and the right. What do these tell us? These tell us the marginal 1d distributions of $X_1$ and $X_2$. The marginal distribution of $X_1$ is the distribution of $X_1$ considering all values of $X_2$ and vice versa. One of the interesting properties of Gaussian distributions is that the marginal distribution of a Gaussian is also a Gaussian distribution. MathematicalMonk on Youtube has a great set of lectures on this topic that I would highly recommend! What would you expect the marginal distribution of $X_1$ to look like? No prizes for guessing. Given $$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mu_1 \\ \mu_2\end{pmatrix} , \begin{pmatrix} a &amp; \rho \\ \rho &amp; b\end{pmatrix} \right)$$ we have the marginal distribution of:$$X_1 \sim \mathcal{N}(\mu_1, a)$$and $$X_2 \sim \mathcal{N}(\mu_2, b)$$       def plot_jointplot_2d(a, b, rho):  data = np. random. multivariate_normal(mean = np. array([0, 0]), cov = np. array([[a, rho], [rho, b]]), size=(10000, ))  data_df = pd. DataFrame(data, columns=[r&#39;$X_1$&#39;,r&#39;$X_2$&#39;])  g = sns. jointplot(x= r&#39;$X_1$&#39;, y=r&#39;$X_2$&#39;, data=data_df, kind=&quot;reg&quot;,color=&#39;gray&#39;)    Ok, let us know try to plot a few jointplots for different covariance matrices. We would be passing in the values of $a$, $b$ and $\rho$ which would make up the covariance matrix as: \begin{pmatrix} a &amp; \rho \\ \rho &amp; b\end{pmatrix} We would make these plots for mean zero.       plot_jointplot_2d(1, 1, -0. 7)    In the plot above, for $a=1$, $b=1$ and $\rho=0. 7$ we can see the negative correlation (but high) between $X_1$ and $X_2$. Let us now increase the variance in $X_1$ and keep all other paramaters constant.       plot_jointplot_2d(2, 1, -0. 7)    One can see from the plot above that the variance in $X_1$ is much higher now and the plot extends from -6 to +6 for $X_1$ while earlier it was restricted from -4 to 4.       plot_jointplot_2d(1, 1, 0. 0)    One can see from the plot above that the correlation between $X_1$ and $X_2$ is zero. Surface plots for bi-variate Gaussian&#182;: We will now look into surface plots for bi-variate Gaussian. This is yet another way to plot and understand Gaussian distributions. I borrow code from an excellent tuorial on plotting bivariate Gaussians.       from scipy. stats import multivariate_normalfrom mpl_toolkits. mplot3d import Axes3Dfrom matplotlib import cmdef make_pdf_2d_gaussian(mu, sigma):  N = 60  X = np. linspace(-3, 3, N)  Y = np. linspace(-3, 4, N)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)  # Create a surface plot and projected filled contour plot under it.   fig = plt. figure()  ax = fig. gca(projection=&#39;3d&#39;)  ax. plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,          cmap=cm. Greys)    ax. set_xlabel(r&quot;$X_1$&quot;)  ax. set_ylabel(r&quot;$X_2$&quot;)  ax. set_zlabel(&quot;PDF&quot;)  cset = ax. contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-0. 15, cmap=cm. Greys)  # Adjust the limits, ticks and view angle  ax. set_zlim(-0. 15,0. 25)  ax. set_zticks(np. linspace(0,0. 2,5))  ax. view_init(27, -15)  ax. set_title(f&#39;$\mu$ = {mu}\n $\Sigma$ = {sigma}&#39;)          mu = np. array([0. , 0. ])sigma = np. array([[ 1. , -0. 5], [-0. 5, 1]])make_pdf_2d_gaussian(mu, sigma)    From the plot above, we can see the surface plot showing the probability density function for the Gaussian with mean \begin{pmatrix} 0 \\ 0\end{pmatrix} and covariance matrix: \begin{pmatrix} 1 &amp; -0. 5 \\ -0. 5 &amp; 1\end{pmatrix} It can be seen that the probability peaks arounds $X_1=0$ and $X_2=0$. The bottom plot shows the same concept using contour plots which we will heavily use from now on. The different circles in the bottom contour plot denote the loci of same probability density. Since the contour plot requires a lesser dimension, it will be easier to use in our further analysis. Also, from the contour plots, we can see the correlation between $X_1$ and $X_2$.       mu = np. array([0. , 0. ])sigma = np. array([[ 1. , 0], [0, 1]])make_pdf_2d_gaussian(mu, sigma)    In the plot above, we can see that $X_1$ and $X_2$ are not correlated. Contour plots for 2D Gaussians&#182;: Having seen the relationship between the surface plots and the contour plots, we will now exclusively focus on the contour plots. Here is a simple function to generate the contour plot for 2g gaussian with mean and covariance as the arguments.       def plot_2d_contour_pdf(mu, sigma):  X = np. linspace(-3, 3, 60)  Y = np. linspace(-3, 4, 60)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)  plt. xlabel(r&quot;$X_1$&quot;)  plt. ylabel(r&quot;$X_2$&quot;)    plt. title(f&#39;$\mu$ = {mu}\n $\Sigma$ = {sigma}&#39;)  plt. contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-0. 15, cmap=cm. Greys)  plt. colorbar()  format_axes(plt. gca())          mu = np. array([0. , 0. ])sigma = np. array([[ 1. , 0. 5], [0. 5, 1. ]])plot_2d_contour_pdf(mu, sigma)  /home/nipunbatra-pc/anaconda3/lib/python3. 7/site-packages/matplotlib/contour. py:1000: UserWarning: The following kwargs were not used by contour: &#39;zdir&#39;, &#39;offset&#39; s)  The plot above shows the contour plot for 2d gaussian with mean [0, 0] and covariance [[ 1. , 0. 5], [0. 5, 1. ]]. We can see the correlation between $X_1$ and $X_2$ Sample from 2d gaussian and visualising it on XY plane&#182;: We will now sample a point from a 2d Gaussian and describe a new way of visualising it.  The left most plot shows the covariance matrix. The middle plot shows the contour plot. The dark point marked in the contour plot is a sampled point (at random) from this 2d Gaussian distribution. The right most plot is an alternative representation of the sampled point. The x-axis corresponds to the labels $X_1$ and $X_2$ and the corresponding y-axis are the coordinates of the point in the $X_1$, $X_2$ dimension shown in the contour plot. We will now write a function to generate a random sample from a 2d gaussian given it's mean and covariance matrix.       def plot_2d_contour_pdf_dimensions(mu, sigma, random_num):  fig, ax = plt. subplots(ncols=3, figsize=(12, 4))  X = np. linspace(-3, 3, 60)  Y = np. linspace(-3, 3, 60)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)  random_point = F. rvs(random_state=random_num)    sns. heatmap(sigma, ax=ax[0], annot=True)  ax[1]. contour(X, Y, Z, cmap=cm. Greys)  ax[1]. scatter(random_point[0], random_point[1], color=&#39;k&#39;,s=100)  ax[1]. set_xlabel(r&quot;$X_1$&quot;)  ax[1]. set_ylabel(r&quot;$X_2$&quot;)    data_array = pd. Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;])  data_array. plot(ax=ax[2], kind=&#39;line&#39;, marker=&#39;o&#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[2]. set_ylim(-3, 3)    format_axes(ax[0])  format_axes(ax[1])  format_axes(ax[2])  ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[1]. set_title(&quot;Contour of pdf&quot;)  ax[2]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images&quot;):    os. makedirs(&quot;images&quot;)  if not os. path. exists(f&quot;images/{sigma[0, 1]}&quot;):    os. makedirs(f&quot;images/{sigma[0, 1]}&quot;)  plt. savefig(f&quot;images/{sigma[0, 1]}/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()    We will now create 20 such samples and animate them       for i in range(20):  plot_2d_contour_pdf_dimensions( mu, np. array([[ 1. , 0. 1], [0. 1, 1. ]]), i)          !convert -delay 20 -loop 0 images/0. 1/*. jpg sigma-0-1. gif     Since the correlation between the two variables $X_1$ and $X_2$ was low (0. 1), we can the see that rightmost plot jumping a lot, i. e. to say that the values of $X_1$ and $X_2$ are not tighly constrained to move together.       for i in range(20):  plot_2d_contour_pdf_dimensions( mu, np. array([[ 1. , 0. 7], [0. 7, 1. ]]), i)          !convert -delay 20 -loop 0 images/0. 7/*. jpg sigma-0-7. gif     The above GIF shows the same plot/animation for the 2d Gaussian where the correlation between the two variables is high (0. 7). Thus, we can see that the two variables tend to move up and down together. Conditional Bivariate Distribution&#182;: All excellent till now. Now, let us move to the case in which some variable's values are known. We would then look to find the distribution of the other variables conditional on the value of the known variable. I borrow some text from Wikipedia on the subject. $$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} 0 \\ 0\end{pmatrix} , \begin{pmatrix} 1 &amp; \rho \\ \rho &amp; 1\end{pmatrix} \right)$$The conditional expectation of $X_2$ given $X_1$ is: $\operatorname{E}(X_2 \mid X_1=x_1)= \rho x_1 $ and the conditional variance is: $\operatorname{var}(X_2 \mid X_1 = x_1) = 1-\rho^2$ So, the question now is: suppose we fix $X_1 = 1$, what is the distribution of $X_2$. Again, Gaussians are amazing - the conditional distributionon is again a Gaussian. Let us make some plots to understand better. The following plots would be showing the distribution of $X_2$ with fixed $X_1$       def plot_2d_contour_pdf_dimensions_fixed_x1(sigma, random_num, x1 = 1):  mu = np. zeros(2)  fig, ax = plt. subplots(ncols=3, figsize=(12, 4))  X = np. linspace(-3, 3, 60)  Y = np. linspace(-3, 3, 60)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)    rho = sigma[0, 1]  F_cond_x1 = multivariate_normal(rho*x1, 1-rho**2)  random_point_x2 = F_cond_x1. rvs(random_state=random_num)  sns. heatmap(sigma, ax=ax[0], annot=True)  ax[1]. contour(X, Y, Z, cmap=cm. Greys)  ax[1]. scatter(x1, random_point_x2, color=&#39;k&#39;,s=100)  ax[1]. set_xlabel(r&quot;$X_1$&quot;)  ax[1]. set_ylabel(r&quot;$X_2$&quot;)    data_array = pd. Series([x1, random_point_x2], index=[&#39;X1&#39;,&#39;X2&#39;])  data_array. plot(ax=ax[2], kind=&#39;line&#39;, color=&#39;k&#39;)  ax[2]. scatter(x=0, y=x1, color=&#39;red&#39;, s=100)  ax[2]. scatter(x=1, y=random_point_x2, color=&#39;k&#39;, s=100)    plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[2]. set_ylim(-3, 3)  format_axes(ax[0])  format_axes(ax[1])  format_axes(ax[2])  ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[1]. set_title(&quot;Contour of pdf&quot;)  ax[2]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/conditional/&quot;):    os. makedirs(&quot;images/conditional/&quot;)  if not os. path. exists(f&quot;images/conditional/{sigma[0, 1]}&quot;):    os. makedirs(f&quot;images/conditional/{sigma[0, 1]}&quot;)  plt. savefig(f&quot;images/conditional/{sigma[0, 1]}/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          for i in range(20):  plot_2d_contour_pdf_dimensions_fixed_x1(np. array([[ 1. , 0. 1], [0. 1, 1. ]]), i)          !convert -delay 20 -loop 0 images/conditional/0. 1/*. jpg conditional-sigma-0-1. gif     The above animation shows the movement of $X_2$ with $X_1=1$. The $X_1=1$ is shown in red in the righmost plot. In the middle plot, we can confirm that the movement is only in the $X_2$ dimension. Further, since the correlation between $X_1$ and $X_2$ is weak, the righmost plot seems to wiggle or jump a lot!       for i in range(20):  plot_2d_contour_pdf_dimensions_fixed_x1(np. array([[ 1. , 0. 7], [0. 7, 1. ]]), i)          !convert -delay 20 -loop 0 images/conditional/0. 7/*. jpg conditional-sigma-0-7. gif     In the plot above, we repeat the same p|rocedure but with a covariance matrix having a much higher correlation between $X_1$ and $X_2$. From the righmost plot, we can clearly see that the jumps in $X2$ are far lesser. This is expected, since the two variables are correlated! Visualising the same procedure for 5 dimensional Gaussian&#182;: We will now repeat the same procedure we did for 2d case in 5 dimensions.       covariance_5d = np. array([[1, 0. 9, 0. 8, 0. 6, 0. 4],             [0. 9, 1, 0. 9, 0. 8, 0. 6],             [0. 8, 0. 9, 1, 0. 9, 0. 8],             [0. 6, 0. 8, 0. 9, 1, 0. 9],             [0. 4, 0. 6, 0. 8, 0. 9, 1]])          def plot_5d_contour_pdf_dimensions(cov, random_num):  fig, ax = plt. subplots(ncols=2, figsize=(6, 3))  mu = np. zeros(5)  F = multivariate_normal(mu, cov)  random_point = F. rvs(random_state=random_num)    sns. heatmap(cov, ax=ax[0], annot=True)      data_array = pd. Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;, &#39;X5&#39;])  data_array. plot(ax=ax[1], kind=&#39;line&#39;, marker=&#39;o&#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[1]. set_ylim(-3, 3)  for i in range(2):    format_axes(ax[i])    ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[-1]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/5d/&quot;):    os. makedirs(&quot;images/5d&quot;)    plt. savefig(f&quot;images/5d/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          plot_5d_contour_pdf_dimensions(covariance_5d, 2)          for i in range(20):  plot_5d_contour_pdf_dimensions(covariance_5d, i)          !convert -delay 20 -loop 0 images/5d/*. jpg 5d. gif     From the visualisation above we can see that: since X1 and X2 are highly correlated, they move up and down togetherbut, X1 and X5 have low correlation, thus, they can seem to wiggle almost independently of each other. We are now getting somewhere. If the correlation between the variables is very high, we will get a smooth curve joining them. Right? Almost getting to the point where we can draw the introductory plot shown at the top of the post. Conditional Multivariate Distribution&#182;: Ok, now let us draw the conditional distribution over this higher 5d space. We will fix the values of some of the variables and see the distribution of the others. Borrowing from Wikipedia If $N$-dimensional $x$ is partitioned as follows $$\mathbf{x}=\begin{bmatrix} \mathbf{x}_A \\ \mathbf{x}_B\end{bmatrix}\text{ with sizes }\begin{bmatrix} q \times 1 \\ (N-q) \times 1 \end{bmatrix}$$and accordingly $μ$ and $Σ$ are partitioned as follows $$\boldsymbol\mu=\begin{bmatrix} \boldsymbol\mu_A \\ \boldsymbol\mu_B\end{bmatrix}\text{ with sizes }\begin{bmatrix} q \times 1 \\ (N-q) \times 1 \end{bmatrix}$$$$\boldsymbol\Sigma=\begin{bmatrix} \boldsymbol\Sigma_{AA} &amp; \boldsymbol\Sigma_{AB} \\ \boldsymbol\Sigma_{BA} &amp; \boldsymbol\Sigma_{BB}\end{bmatrix}\text{ with sizes }\begin{bmatrix} q \times q &amp; q \times (N-q) \\ (N-q) \times q &amp; (N-q) \times (N-q) \end{bmatrix}$$then the distribution of $x_A$ conditional on $x_B=b$ is multivariate normal $(x_A|x_B=b)\sim \mathcal{N}(\bar{\mu}, \bar{\Sigma})$ $$\bar{\boldsymbol\mu}=\boldsymbol\mu_A + \boldsymbol\Sigma_{AB} \boldsymbol\Sigma_{BB}^{-1}\left( \mathbf{B} - \boldsymbol\mu_B\right)$$and covariance matrix $$\overline{\boldsymbol\Sigma}=\boldsymbol\Sigma_{AA} - \boldsymbol\Sigma_{AB} \boldsymbol\Sigma_{BB}^{-1} \boldsymbol\Sigma_{BA}. $$Let us for our example take $X_5 = -2$. We have: $x_A = [x_1, x_2, x_3, x_4]$ and $x_B = [x_5]$ Assuming the covariance matrix of size 5 X 5 is referred as $C$ $$\boldsymbol\Sigma_{AA}=\begin{bmatrix} C_{11} &amp; C_{12} &amp; C_{13} &amp; C_{14}\\ C_{21} &amp; C_{22} &amp; C_{23} &amp; C_{24}\\ C_{31} &amp; C_{32} &amp; C_{33} &amp; C_{34}\\ C_{41} &amp; C_{42} &amp; C_{43} &amp; C_{44}\\\end{bmatrix} \\$$$$\boldsymbol\Sigma_{AB}=\begin{bmatrix} C_{15}\\ C_{25}\\ C_{35}\\ C_{45}\\\end{bmatrix}$$$$\boldsymbol\Sigma_{BA}=\begin{bmatrix} C_{51}&amp; C_{52} &amp; C_{53} &amp; C_{54}\\\end{bmatrix}$$$$\boldsymbol\Sigma_{BB}=\begin{bmatrix} C_{55}\\\end{bmatrix}$$Putting in the numbers we get:       sigma_AA = covariance_5d[:4, :4]          sigma_AA  array([[1. , 0. 9, 0. 8, 0. 6],    [0. 9, 1. , 0. 9, 0. 8],    [0. 8, 0. 9, 1. , 0. 9],    [0. 6, 0. 8, 0. 9, 1. ]])        sigma_AB = covariance_5d[:4, 4]. reshape(-1, 1)          sigma_AB  array([[0. 4],    [0. 6],    [0. 8],    [0. 9]])        sigma_BA = covariance_5d[4, :4]. reshape(1, -1)          sigma_BA  array([[0. 4, 0. 6, 0. 8, 0. 9]])        sigma_BB = covariance_5d[4, 4]. reshape(-1, 1)          sigma_BB  array([[1. ]])  Now, calculating $\bar{\mu}$       mu_bar = np. zeros((4, 1)) + sigma_AB@np. linalg. inv(sigma_BB)*(-2)          mu_bar  array([[-0. 8],    [-1. 2],    [-1. 6],    [-1. 8]])  Since, $x_5$ has highest correlation with $x_4$ it makes sense for $x_5=-2$ to have the mean of $x_4$ to be close to -2. Now, calculating $\bar{\Sigma}$       sigma_bar = sigma_AA - sigma_AB@np. linalg. inv(sigma_BB)@sigma_BA          sigma_bar  array([[0. 84, 0. 66, 0. 48, 0. 24],    [0. 66, 0. 64, 0. 42, 0. 26],    [0. 48, 0. 42, 0. 36, 0. 18],    [0. 24, 0. 26, 0. 18, 0. 19]])  Now, we have the new mean and covariance matrices for $x_A = [x_1, x_2, x_3, x_4]$ and $x_B = [x_5] = [-2]$. Let us now draw some samples fixing $x_5 = -2$       cov = sigma_barmu = mu_bar. flatten()def plot_5d_samples_fixed_x2(random_num):  fig, ax = plt. subplots(ncols=2, figsize=(6, 3))      F = multivariate_normal(mu, cov)    sns. heatmap(cov, ax=ax[0], annot=True)  random_point = F. rvs(random_state=random_num)      data_array = pd. Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;])  data_array[&#39;X5&#39;] = -2  data_array. plot(ax=ax[1], kind=&#39;line&#39;, marker=&#39;. &#39;,color=&#39;k&#39;)  plt. scatter([4], [-2], color=&#39;red&#39;, s=100)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[1]. set_ylim(-3, 3)  for i in range(2):    format_axes(ax[i])    ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[-1]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/5d/conditional/1&quot;):    os. makedirs(&quot;images/5d/conditional/1&quot;)    plt. savefig(f&quot;images/5d/conditional/1/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()            for i in range(20):  plot_5d_samples_fixed_x2(i)          !convert -delay 20 -loop 0 images/5d/conditional/1/*. jpg 5d-conditional-1. gif     Let's increase to 20 dimensions now!&#182;: We can not surely write the covariance matrix for 20 dimensions. Let us use a small trick called the kernel function to create this matrix. We will come it later. For now, let us think of this function as a function which: outputs low numbers for $x_1$ and $x_2$ if they differ by a lotoutputs high number for $x_1$ and $x_2$ if they are very close      def rbf_kernel(x_1, x_2, sig):  return np. exp((-(x_1-x_2)**2)/2*(sig**2))          rbf_kernel(1, 1, 0. 4)  1. 0  Since 1=1, the above function evaluates to 1 showing that 1 is similar to 1       rbf_kernel(1, 2, 0. 4)  0. 9231163463866358  Since 1 and 2 are close, the function above evaluates to close to 1       rbf_kernel(1, 2, 1)  0. 6065306597126334  Ok, we use the same first two arguments 1 and 2 but change the last one to 1 from 0. 4 and we see that the function evaluates to a much smaller number. Thus, we can see that increase the sig parameter leads to quicker dropoff in similarity between pair of points. Or, in other words, higher sig means that the influence of a point x_1 reduces quicker. Let us now create the covariance matrix of size (20, 20) using this kernel function.       C = np. zeros((20, 20))          for i in range(20):  for j in range(20):    C[i, j] = rbf_kernel(i, j, 0. 5)    Let us plot the heatmap of the covariance matrix       sns. heatmap(C)  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d3922fd0&gt;  The above heatmap confirms that there is correlation between nearby points, but close to zero or zero correlation otherwise. Let us draw some samples from this 20 dimensional Gaussian&#182;:       def plot_20d_samples(random_num):  fig, ax = plt. subplots(figsize=(10, 3))      F = multivariate_normal(np. zeros(20), C)  random_point = F. rvs(random_state=random_num)  index = [f&#39;X{i}&#39; for i in range(1, 21)]  data_array = pd. Series(random_point, index=index)  data_array. plot(ax=ax, kind=&#39;line&#39;, marker=&#39;. &#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)    plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/20d/&quot;):    os. makedirs(&quot;images/20d/&quot;)    plt. ylim(-3, 3)  plt. savefig(f&quot;images/20d/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          for i in range(50):  plot_20d_samples(i)          !convert -delay 20 -loop 0 images/20d/*. jpg 20d. gif     From the animation above, we can see different family of functions of mean zero across these 20 points. We're really getting close now! Let us now condition on a few elements&#182;: We will create a new ordering of these variables such that the known variables occur towards the end. This allows for easy calculations for conditioning.       order = [2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 5, 10]          new_C = np. zeros_like(C)          old_order = range(20)          for i in range(20):  for j in range(20):    new_C[i, j] = C[order[i], order[j]]          sns. heatmap(new_C, xticklabels=order, yticklabels=order, cmap=&#39;jet&#39;)  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d48a86d8&gt;  Now, we can condition on (x1 = 1, x2 = 3, x6 = -3, X11 = 1). We will use the same procedure we used above in the case of 5d.       B = np. array([1, 3, -3, 1]). reshape(-1, 1)B  array([[ 1],    [ 3],    [-3],    [ 1]])        sigma_AA_20d = new_C[:-B. size, :-B. size]sigma_AA_20d. shape  (16, 16)        sigma_BB_20d = new_C[-B. size:, -B. size:]sigma_BB_20d. shape  (4, 4)        sigma_AB_20d = new_C[:-B. size, -B. size:]sigma_AB_20d. shape  (16, 4)        sigma_BA_20d = new_C[-B. size:, :-B. size]sigma_BA_20d. shape  (4, 16)        mu_bar_20d = np. zeros((20-B. size, 1)) + sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@(B)          sigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@sigma_BA_20d          sns. heatmap(sigma_bar_20d, xticklabels=order[:-B. size], yticklabels=order[:-B. size], cmap=&#39;jet&#39;)  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d9a90e48&gt;        def plot_20d_samples_known_x(random_num):  fig, ax = plt. subplots(figsize=(10, 3))      F = multivariate_normal(mu_bar_20d. flatten(), sigma_bar_20d)  random_point = F. rvs(random_state=random_num)  index = [f&#39;X{i+1}&#39; for i in order[:-B. size]]  data_array = pd. Series(random_point, index=index)  data_array[&#39;X1&#39;] = 1  data_array[&#39;X2&#39;] = 3  data_array[&#39;X6&#39;] = -3  data_array[&#39;X11&#39;] = -1    data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]]  data_array. plot(ax=ax, kind=&#39;line&#39;, marker=&#39;. &#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  plt. scatter([0, 1,5, 10], [1, 3, -3, -1], color=&#39;red&#39;,s=100)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/20d/conditional/&quot;):    os. makedirs(&quot;images/20d/conditional/&quot;)  plt. grid()  plt. ylim(-4, 4)  plt. savefig(f&quot;images/20d/conditional/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          for i in range(50):  plot_20d_samples_known_x(i)          !convert -delay 20 -loop 0 images/20d/conditional/*. jpg 20d-conditional. gif     From the plot above, we can see the known points in red and the other points wiggle to show the families of functions that we fit. Let us now draw a lot of samples and plot the mean and variance in these samples for the unknown X variables. We could have obtained the mean and variance directly using Gaussian marginalisation, but, for now let us just draw many samples.       F = multivariate_normal(mu_bar_20d. flatten(), sigma_bar_20d)dfs = {}for random_num in range(100):  random_point = F. rvs(random_state=random_num)  index = [f&#39;X{i+1}&#39; for i in order[:-B. size]]  data_array = pd. Series(random_point, index=index)  data_array[&#39;X1&#39;] = 1  data_array[&#39;X2&#39;] = 3  data_array[&#39;X6&#39;] = -3  data_array[&#39;X11&#39;] = -1    data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]]  dfs[random_num] = data_array          fig, ax = plt. subplots(figsize=(10, 3))pd. DataFrame(dfs). mean(axis=1). plot(yerr=pd. DataFrame(dfs). std(axis=1),marker=&#39;o&#39;, color=&#39;k&#39;)plt. xticks(np. arange(len(data_array. index)), data_array. index. values)plt. scatter([0, 1,5, 10], [1, 3, -3, -1], color=&#39;red&#39;,s=100)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d37b2358&gt;  From the plot above, we can see the uncertainty (standard deviation) and the mean values for different variables. As expected, the uncertainty close to the known points (red) is low. Also, owing to the smooth nature of the covariance function we can see the means of unknown points close to known points are fairly similar. To summarise: We can very clearly see that there is low variance in zones where we have the known values and high variance otherwise. The farther we go away from a known value, the more is the variance! Kernels!&#182;: We will now take a small plunge into the world of kernels. As mentioned earlier, we will limit the discussion to generating to covariance matrix. We will be redefining the function mentioned above to include two parameters l and s s is the scale of variancel is the influence of the point to neighbouring points      def sig(x1, x2, l, s):  return s**2*(np. exp((-1/2*(l**2))*((x1-x2)**2)))          Cov_matrix = np. zeros((100, 100))          fig, ax = plt. subplots(ncols=4, sharex=True, sharey=True)s = 1for ix, l in enumerate([0. 001, 0. 01, 0. 1, 1]):  for i in range(100):    for j in range(100):      Cov_matrix[i, j] = sig(i, j, l, 1)  im = ax[ix]. imshow(Cov_matrix, cmap=&#39;jet&#39;)  ax[ix]. set_title(f&quot;l={l}&quot;)fig. subplots_adjust(right=0. 8)cbar_ax = fig. add_axes([0. 85, 0. 35, 0. 05, 0. 3])fig. colorbar(im, cax=cbar_ax)plt. suptitle(f&quot;Covariance matrix for varying l and s = {s}&quot;)  Text(0. 5, 0. 98, &#39;Covariance matrix for varying l and s = 1&#39;)  In the plot above, we can the covariance matrices for fixed s=1 and varying l. It can be seen that for very low l, the correlations between far away points is also significant. At l=1, this ceases to be the case.       fig, ax = plt. subplots(ncols=4, sharex=True, sharey=True, figsize=(12, 3))for ix, s in enumerate([1, 10, 20, 30]):  for i in range(100):    for j in range(100):      Cov_matrix[i, j] = sig(i, j, 0. 1, s)  sns. heatmap(Cov_matrix, cmap=&#39;jet&#39;, ax=ax[ix])  ax[ix]. set_title(f&quot;s={s}&quot;)plt. suptitle(&quot;Covariance matrix for varying s and l = 0. 1&quot;)  Text(0. 5, 0. 98, &#39;Covariance matrix for varying s and l = 0. 1&#39;)  Ok, this is great. We can see the different scales on the colorbars with increasing s and fixing l Now, let us try and redo the 20 point dataset with varying kernel parameters with conditioning on some known data.       def fit_plot_gp(kernel_s, kernel_l, known_data, total_data_points, save=False):  &quot;&quot;&quot;  kernel_s: sigma^2 param of kernel  kernel_l: l (width) param of kernel  known_data: {pos: value}  total_data_points  &quot;&quot;&quot;  o = list(range(20))  for key in known_data. keys():    o. remove(key)  o. extend(list(known_data. keys()))    C = np. zeros((total_data_points, total_data_points))  for i in range(total_data_points):    for j in range(total_data_points):      C[i, j] = sig(i, j, kernel_l, kernel_s)        # Making known variables shift  new_C = np. zeros_like(C)  for i in range(20):    for j in range(20):      new_C[i, j] = C[o[i], o[j]]  B = np. array(list(known_data. values())). reshape(-1, 1)    sigma_BA_20d = new_C[-B. size:, :-B. size]  sigma_AB_20d = new_C[:-B. size, -B. size:]  sigma_BB_20d = new_C[-B. size:, -B. size:]  sigma_AA_20d = new_C[:-B. size, :-B. size]  mu_bar_20d = np. zeros((20-B. size, 1)) + sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@(B)  sigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@sigma_BA_20d  F = multivariate_normal(mu_bar_20d. flatten(), sigma_bar_20d)  dfs = {}  for random_num in range(100):    random_point = F. rvs(random_state=random_num)    index = [f&#39;X{i+1}&#39; for i in o[:-B. size]]    data_array = pd. Series(random_point, index=index)    for k, v in known_data. items():      data_array[f&#39;X{k+1}&#39;] = v        data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]]    dfs[random_num] = data_array  fig, ax = plt. subplots(figsize=(10, 3))  mean_vector = pd. DataFrame(dfs). mean(axis=1)  mean_vector. plot(marker=&#39;. &#39;, color=&#39;k&#39;)  yerr=pd. DataFrame(dfs). std(axis=1)    plt. fill_between(range(len(mean_vector)), mean_vector+yerr, mean_vector-yerr, color=&#39;gray&#39;,alpha=0. 4)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  plt. scatter(list(known_data. keys()), list(known_data. values()), color=&#39;gray&#39;,s=200,zorder=1)  format_axes(plt. gca())  plt. title(f&quot; l = {kernel_l} and s = {kernel_s}&quot;)  import os  if save:    if not os. path. exists(&quot;images/20d/conditional-points/&quot;):      os. makedirs(&quot;images/20d/conditional-points/&quot;)    plt. grid()    plt. xticks(np. arange(len(data_array. index)), np. arange(len(data_array. index)))    plt. ylim(-4, 4)    plt. title(f&quot;Known data: {known_data}&quot;)    plt. savefig(f&quot;images/20d/conditional-points/{len(known_data. keys())}. jpg&quot;, bbox_inches=&quot;tight&quot;)    plt. close()              known_d = {0:-2, 1:3, 9:-1, 14:-1}          fit_plot_gp(1, 0. 5, known_d, 20)    The above plot shows the uncertainty and the family of functions for l=0. 5 and s=1.       fit_plot_gp(5, 0. 5, known_d, 20)    Keeping l=0. 5, the above plot shows how increasing s increases the uncertainty of estimation.       fit_plot_gp(1, 1, known_d, 20)    The above plot shows how increasing l reduces the influence between far away points.       fit_plot_gp(1, 100, known_d, 20)    The above plot increases l to a very large value. Seems to be just moving around the mean?       np. random. seed(0)order_points_added = np. random. choice(range(20), size=9, replace=False)k = {}for i in range(9):  k[order_points_added[i]] = np. random. choice(range(-3, 3))  fit_plot_gp(1, 0. 5, k, 20, True)          !convert -delay 40 -loop 0 images/20d/conditional-points/*. jpg 20d-conditional-main. gif    Let us create a small animation where we keep on adding points and see how the uncertainty and estimation changes Creating a scikit-learn like function containing fit and predict&#182;: I'll now bring in the formal definitions, summarise the discussion and write a function akin to scikit-learn which can accept train data to estimate for test data. Formally defining GPs&#182;: A Gaussian process is fully specified by a mean function m(x) andcovariance function K(x, x'): $$f(x) \sim GP (m(x),K(x, x')$$Let us consider a case of noiseless GPs now Noiseless GPs&#182;: Given train data $$D = {(x_i, y_i), i = 1:N}$$ Given a test set $X_{*}$ of size $N_* \times d $ containing $N_*$ points in ${\rm I\!R}^d$, we want to predict function outputs $y_{*}$ We can write: $$\begin{pmatrix} y \\ y_*\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mu \\ \mu_*\end{pmatrix} , \begin{pmatrix} K &amp; K_* \\ K_*^T &amp; K_{**}\end{pmatrix} \right)$$where $$K = Ker(X, X) \in {\rm I\!R}^{N\times N}\\K_* = Ker(X, X_*) \in {\rm I\!R}^{N\times N_*}\\K_{**} = Ker(X_*, X_*) \in {\rm I\!R}^{N_*\times N_*}\\$$We had previously used the kernel which we will continue to use def sig(x1, x2, l, s):  return s**2*(np. exp((-1/2*(l**2))*((x1-x2)**2)))We can then write: $$p(y_*|X_*, X, y) \sim \mathcal{N}(\mu', \Sigma') \\\mu' = \mu_* + K_*^TK^{-1}(x-\mu) \\\Sigma' = K_{**} - K_*^TK^{-1}K_*$$      class NoiselessGP_inversion:  def __init__(self, l=0. 1, s=1, prior_mean=0):    self. l = l    self. s = s       self. prior_mean = prior_mean      def prior_sample(self, x, n):    &quot;&quot;&quot;    Sample GP on x    &quot;&quot;&quot;    self. sample_k = self. create_cov_matrix(x, x, self. l, self. s)    for i in range(n):      pass       def kernel(self, a, b, l, s):    &quot;&quot;&quot;    Borrowed from Nando De Freita&#39;s lecture code    https://www. cs. ubc. ca/~nando/540-2013/lectures/gp. py    &quot;&quot;&quot;    sqdist = np. sum(a**2,1). reshape(-1,1) + np. sum(b**2,1) - 2*np. dot(a, b. T)    return s**2*np. exp(-. 5 * (1/l) * sqdist)    def fit(self, train_x, train_y):    self. train_x = train_x    self. train_y = train_y    self. N = len(train_x)    self. K = self. kernel(train_x, train_x, self. l, self. s)              def predict(self, test_x):    self. N_star = len(test_x)    self. K_star = self. kernel(self. train_x, test_x, self. l, self. s)    self. K_star_star = self. kernel(test_x, test_x, self. l, self. s)    self. posterior_mu = self. prior_mean + self. K_star. T@np. linalg. inv(self. K)@(self. train_y-self. prior_mean)    self. posterior_sigma = self. K_star_star - self. K_star. T@np. linalg. inv(self. K)@self. K_star    return self. posterior_mu, self. posterior_sigma          clf = NoiselessGP_inversion()          train_x = np. array([-4, -3, -2, -1, 1]). reshape(5,1)train_y = np. sin(train_x)test_x = np. linspace(-5, 5, 50). reshape(-1, 1)test_y = np. sin(test_x)          plt. plot(train_x, train_y,&#39;ko-&#39;)  [&lt;matplotlib. lines. Line2D at 0x7fb2d954f198&gt;]        clf. fit(train_x, train_y)          posterior_mu, posterior_var = clf. predict(test_x)          plt. plot(test_x, clf. posterior_mu,&#39;k&#39;,label=&#39;Predicted&#39;,lw=1)plt. plot(test_x, test_y, &#39;purple&#39;,label=&#39;GT&#39;,lw=2)plt. plot(train_x, train_y, &#39;ko&#39;,label=&#39;Training Data&#39;)plt. fill_between(test_x. flatten(),         (clf. posterior_mu. flatten() - clf. posterior_sigma. diagonal(). flatten()),         (clf. posterior_mu. flatten() + clf. posterior_sigma. diagonal(). flatten()),         color=&#39;gray&#39;, alpha=0. 3        )plt. legend()format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d9516518&gt;  Cholesky decomposition&#182;: We had previously used matrix inversion to do the computation for computing the posterior mean and variance in our GP. However, the matrices involved may be poorly conditioned and thus Cholesky decomposition is often favoured. From Wikipedia, the Cholesky decomposition of a matrix $A$ is given as:$$\mathbf{A} = \mathbf{L L}^T$$ where $L$ is a real lower triangular matrix. We can thus re-write the posterior mean and covariance as: $$p(y_*|X_*, X, y) \sim \mathcal{N}(\mu', \Sigma') \\K = LL^T \\$$We are now going to use the \ as follows:if $A\omega = B$, then $\omega$ = $B$ \ $A$ We now have:$$\alpha = K^{-1}(x-\mu) \\or, \alpha = {LL^T}^{-1}(x-\mu) \\or, \alpha = L^{-T}L^{-1}(x-\mu) \\Let, K^{-1}(x-\mu) = \beta \\Thus, L^{-T}L^{-1}(x-\mu) = \beta \\Let, L^{-1}(x-\mu) = \gamma\\Thus, L\gamma = x-\mu \\Thus, \gamma = L \setminus (x-\mu)\\\Thus, \alpha = L^{T} \setminus (L \setminus (x-\mu))$$ In Python, the same can be written as: L = np. linalg. cholesky(K)  alpha = np. linalg. solve(L. T, np. linalg. solve(L, x-mu))Thus, we can find the posterior mean as:$$\mu' = \mu_* + K_*^T \alpha \\$$ We also know that$$\Sigma' = K_{**} - K_*^TK^{-1}K_*$$ Let us now define$$v = L \setminus K_{*}\\or, v = L^{-1}K_{*}\\Thus, v^{T} = K_{*}^TL^{-T}\\Thus, v^{T}v = K_{*}^TL^{-T}L^{-1}K_{*}\\Thus, v^{T}v = K_*^TK^{-1}K_* = K_{**} - \Sigma' $$ Let us know rewrite the code with Cholesky decomposition.       class NoiselessGP_Cholesky:  def __init__(self, l=0. 1, s=1, prior_mean=0):    self. l = l    self. s = s       self. prior_mean = prior_mean      def prior_sample(self, x, n):    &quot;&quot;&quot;    Sample GP on x    &quot;&quot;&quot;    self. sample_k = self. create_cov_matrix(x, x, self. l, self. s)    for i in range(n):      pass       def kernel(self, a, b, l, s):    &quot;&quot;&quot;    Borrowed from Nando De Freita&#39;s lecture code    https://www. cs. ubc. ca/~nando/540-2013/lectures/gp. py    &quot;&quot;&quot;    sqdist = np. sum(a**2,1). reshape(-1,1) + np. sum(b**2,1) - 2*np. dot(a, b. T)    return s**2*np. exp(-. 5 * (1/l) * sqdist)    def fit(self, train_x, train_y):    self. train_x = train_x    self. train_y = train_y    self. N = len(train_x)    self. K = self. kernel(train_x, train_x, self. l, self. s)    self. L = np. linalg. cholesky(self. K)              def predict(self, test_x):    self. N_star = len(test_x)    self. K_star = self. kernel(self. train_x, test_x, self. l, self. s)    self. K_star_star = self. kernel(test_x, test_x, self. l, self. s)    self. alpha = np. linalg. solve(self. L. T, np. linalg. solve(self. L, self. train_y-self. prior_mean))    self. v = np. linalg. solve(self. L, self. K_star)    self. posterior_mu = self. prior_mean + self. K_star. T@self. alpha    self. posterior_sigma = self. K_star_star - self. v. T@self. v    return self. posterior_mu, self. posterior_sigma          clf = NoiselessGP_Cholesky()clf. fit(train_x, train_y)posterior_mu_cholesky, posterior_var_cholesky = clf. predict(test_x)    We will now compare our Cholesky decomposition based decompostion with the earlier one.       np. allclose(posterior_mu_cholesky, posterior_mu)  True        np. allclose(posterior_var_cholesky, posterior_var)  True  Ok, all looks good till now! Let us now move on to the case for Noisy GPs. Noisy GPs&#182;: Previously, we had assumed a noiseless model, which is to say, for the observed data, we had:$$y_i = f(x_i)$$ We now make the model more flexible by saying that there can be noise in the observed data as well, thus:$$y_i = f(x_i) + \epsilon \\\epsilon \sim \mathcal{N}(0, \sigma_y^2)$$ One of the main difference compared to the noiseless model would be that in the noisy model, we will have some uncertainty even about the training points. Everything about our model remains the same, except for the change in the covariance matrix $K$ for the training points, which is now given as: $$K_y = \sigma_y^2\mathbf{I_n} + K$$We can now rewrite the function as follows:       class NoisyGP:  def __init__(self, l = 0. 1, s = 1, prior_mean = 0, sigma_y = 1):    self. l = l    self. s = s       self. prior_mean = prior_mean    self. sigma_y = sigma_y      def prior_sample(self, x, n):    &quot;&quot;&quot;    Sample GP on x    &quot;&quot;&quot;    self. sample_k = self. create_cov_matrix(x, x, self. l, self. s)    for i in range(n):      pass       def kernel(self, a, b, l, s):    &quot;&quot;&quot;    Borrowed from Nando De Freita&#39;s lecture code    https://www. cs. ubc. ca/~nando/540-2013/lectures/gp. py    &quot;&quot;&quot;    sqdist = np. sum(a**2,1). reshape(-1,1) + np. sum(b**2,1) - 2*np. dot(a, b. T)    return s**2*np. exp(-. 5 * (1/l) * sqdist)    def fit(self, train_x, train_y):    self. train_x = train_x    self. train_y = train_y    self. N = len(train_x)    self. K = self. kernel(train_x, train_x, self. l, self. s) + self. sigma_y*np. eye(len(train_x))    self. L = np. linalg. cholesky(self. K)              def predict(self, test_x):    self. N_star = len(test_x)    self. K_star = self. kernel(self. train_x, test_x, self. l, self. s)    self. K_star_star = self. kernel(test_x, test_x, self. l, self. s)    self. alpha = np. linalg. solve(self. L. T, np. linalg. solve(self. L, self. train_y-self. prior_mean))    self. v = np. linalg. solve(self. L, self. K_star)    self. posterior_mu = self. prior_mean + self. K_star. T@self. alpha    self. posterior_sigma = self. K_star_star - self. v. T@self. v    return self. posterior_mu, self. posterior_sigma          clf = NoisyGP(sigma_y=0. 2)clf. fit(train_x, train_y)posterior_mu_noisy, posterior_var_noisy = clf. predict(test_x)          plt. plot(test_x, clf. posterior_mu,&#39;k&#39;,label=&#39;Predicted&#39;,lw=1)plt. plot(test_x, test_y, &#39;purple&#39;,label=&#39;GT&#39;,lw=2)plt. plot(train_x, train_y, &#39;ko&#39;,label=&#39;Training Data&#39;)plt. fill_between(test_x. flatten(),         (clf. posterior_mu. flatten() - clf. posterior_sigma. diagonal(). flatten()),         (clf. posterior_mu. flatten() + clf. posterior_sigma. diagonal(). flatten()),         color=&#39;gray&#39;, alpha=0. 3        )plt. legend()format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d4725978&gt;  We can now see that our model has some uncertainty even on the train points! "
    }, {
    "id": 6,
    "url": "https://nipunbatra.github.io/academia/2018/08/18/Placement-Preparation-2018-1-HashMap.html",
    "title": "Placement-Preparation-2018-1-HashMap",
    "body": "2018/08/18 -           In this blogpost, we will take a question from Cracking the Coding Interview. I discussed this question with Masters students at IITGN. We came up with some great answers. I'll show how we increasingly went towards better solutions starting from naive ones. Problem statement Find all integer solutions to the problem$a^3 + b^3 = c^3 + d^3$ where $1&lt;=a&lt;=n,1&lt;=b&lt;=n,1&lt;=c&lt;=n,1&lt;=d&lt;=n$ First attempt : Naive bruteforce $O(n^4)$&#182;: Let's write a very simple first attempt. We will write four nested loops. This will be $O(n^4)$ solution.       def f1(n):  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        for d in range(1, n+1):          if a**3 + b**3 == c**3 + d**3:            out. append((a, b, c, d))  return out           f1_time = %timeit -o f1(50)  6. 65 s ± 203 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)        f1_time. average  6. 646897936570895  Second attempt : Reduce computations in brute force method&#182;: Let's now try to optimise f1. We will still use a solution of $O(n^4)$ solution. However, we add one small optimisation fo f1. We break from the innermost loop once we find a match. This will hopefull save us some computations.       def f2(n):  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        for d in range(1, n+1):          if a**3 + b**3 == c**3 + d**3:            out. append((a, b, c, d))            break  return out           f2_time = %timeit -o f2(50)  6. 29 s ± 26. 3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  Ok. We're little better than f1. Every reduced computation is time saved! Third attempt : Reduce repeated computations by saving cubes of numbers&#182;: One of the student came up with an excellent observation. Why should we keep on computing cubes of numbers? This is a repeated operation. Let's instead store them in a dictionary.       def f3(n):  cubes = {}  for x in range(1, n+1):    cubes[x] = x**3  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        for d in range(1, n+1):          if cubes[a] + cubes[b] == cubes[c] + cubes[d]:            out. append((a, b, c, d))            break  return out           f3_time = %timeit -o f3(50)  1. 05 s ± 4. 11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  Ok. We now mean business! This is about 6 times quicker than our previous version. Fourth attempt : Reduce one loop $O(n^3)$&#182;: In this solution, we will reduce one loop. We can solve for $d^3 = a^3 + b^3 - c^3$ and find all the integer solutions. Now, there's another clever optimisation that I have added. We can precompute the cubes and the cuberoots corresponding to numbers from 1 to N and perfect cubes from 1 to $N^3$ respectively.       def f4(n):  cubes = {}  cuberoots = {}  for x in range(1, n+1):    x3 = x**3    cubes[x] = x3    cuberoots[x3] = x  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        d3 = (cubes[a] + cubes[b] - cubes[c])        if d3 in cuberoots:          out. append((a, b, c, cuberoots[d3]))  return out           f4_time = %timeit -o f4(50)  21. 7 ms ± 1. 99 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)  This is seriously fast now! Fifth attempt : Reduce another loop $O(n^2)$&#182;: In this solution, we will reduce one more loop. We can compute $a^3 + b^3$ for all a, b. And then find c and d where $c^3 + d^3$ is the same as $a^3 + b^3$. This has a few Python tricks inside! One of the special cases to handle is of the type $1^3 + 2^3 = 2^3 + 1^3$       def f5(n):  out = []  cubes = {}  for x in range(1, n+1):    cubes[x] = x**3    sum_a3_b3 = {}  for a in range(1, n+1):    for b in range(1, n+1):      temp = cubes[a]+cubes[b]      if temp in sum_a3_b3:          sum_a3_b3[temp]. append((a, b))      else:        sum_a3_b3[temp] = [(a, b)]  for c in range(1, n+1):    for d in range(1, n+1):      sum_c3_d3 = cubes[c] + cubes[d]      if sum_c3_d3 in sum_a3_b3:        for (a, b) in sum_a3_b3[sum_c3_d3]:          out. append((a, b, c, d))  return out          f5_time = %timeit -o f5(50)  1. 97 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  Plain Wow! Going from 6 seconds to about 2 ms! Let's plot the timings on a log scale to learn more.       %matplotlib inlineimport matplotlib. pyplot as pltimport pandas as pd          s = pd. Series({&#39;Naive (O(N^4))&#39;:f1_time. average,       &#39;Naive (O(N^4)) with break&#39;:f2_time. average,       &#39;Naive (O(N^4)) with break and precomputing cubes&#39;:f3_time. average,       &#39;(O(N^3))&#39;:f4_time. average,       &#39;(O(N^2))&#39;:f5_time. average})          s. plot(kind=&#39;bar&#39;, logy=True)plt. ylabel(&quot;Time&quot;);    Hope this was fun! "
    }, {
    "id": 7,
    "url": "https://nipunbatra.github.io/academia/2018/08/11/academics-time.html",
    "title": "An illustrative view of how academics spend their time!",
    "body": "2018/08/11 -  "
    }, {
    "id": 8,
    "url": "https://nipunbatra.github.io/sustainability/2018/06/26/map-electricity-access.html",
    "title": "Visualising Electricity Access Over Space and Time",
    "body": "2018/06/26 -           In this post, I'll explore electricity access, i. e. globally what fraction of people have access to electricity. Beyond the goal of finding the electricity access, this post will also serve to illustrate how the coolness coefficient of the Python visualisation ecosystem! I'll be using data from World Bank for electricity access. See the image below for the corresponding page.  Downloading World Bank data&#182;: Now, a Python package called wbdata provides a fairly easy way to access World Bank data. I'd be using it to get data in Pandas DataFrame.       %matplotlib inlineimport pandas as pdimport wbdataimport matplotlib. pyplot as pltimport datetimedata_date = (datetime. datetime(1990, 1, 1), datetime. datetime(2016, 1, 1))df_elec = wbdata. get_data(&quot;EG. ELC. ACCS. ZS&quot;, pandas=True, data_date=data_date)          df_elec. head()  country   dateArab World 2016  88. 768654      2015  88. 517967      2014  88. 076774      2013  88. 389705      2012  87. 288244Name: value, dtype: float64  Downloading Geodata and Reading Using GeoPandas&#182;: I'd now be downloading shapefile data for different countries. This will help us to spatially plot the data for the different countries.       !wget http://naciscdn. org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes. zip  --2018-06-26 15:52:50-- http://naciscdn. org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes. zipResolving naciscdn. org (naciscdn. org). . . 146. 201. 97. 163Connecting to naciscdn. org (naciscdn. org)|146. 201. 97. 163|:80. . . connected. HTTP request sent, awaiting response. . . 200 OKLength: 5077755 (4. 8M) [application/x-zip-compressed]Saving to: ‘ne_10m_admin_0_countries_lakes. zip’ne_10m_admin_0_coun 100%[===================&gt;]  4. 84M  246KB/s  in 22s   2018-06-26 15:53:12 (228 KB/s) - ‘ne_10m_admin_0_countries_lakes. zip’ saved [5077755/5077755]  Extracting shapefile&#182;:       import zipfilezip_ref = zipfile. ZipFile(&#39;ne_10m_admin_0_countries_lakes. zip&#39;, &#39;r&#39;)zip_ref. extractall(&#39;. &#39;)zip_ref. close()          import geopandas as gpdgdf = gpd. read_file(&#39;ne_10m_admin_0_countries_lakes. shp&#39;)[[&#39;ADM0_A3&#39;, &#39;geometry&#39;]]          gdf. head()           ADM0_A3   geometry         0   IDN   (POLYGON ((117. 7036079039552 4. 163414542001791. . .        1   MYS   (POLYGON ((117. 7036079039552 4. 163414542001791. . .        2   CHL   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .        3   BOL   POLYGON ((-69. 51008875199994 -17. 5065881979999. . .        4   PER   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .      Visualising electricity access in 2016&#182;: Getting electricity access data for 2016&#182;:       df_2016 = df_elec. unstack()[[&#39;2016&#39;]]. dropna()          df_2016. head()        date   2016       country            Afghanistan   84. 137138       Albania   100. 000000       Algeria   99. 439568       Andorra   100. 000000       Angola   40. 520607     In order to visualise electricity access data over the map, we would have to join the GeoPandas object gdf and df_elec Joining gdf and df_2016&#182;: Now, gdf uses alpha_3 codes for country names like AFG, etc. , whereas df_2016 uses country names. We will thus use pycountry package to get code names corresponding to countries in df_2016 as shown in this StackOverflow post.       import pycountrycountries = {}for country in pycountry. countries:  countries[country. name] = country. alpha_3codes = [countries. get(country, &#39;Unknown code&#39;) for country in df_2016. index]df_2016[&#39;Code&#39;] = codes          df_2016. head()        date   2016   Code       country               Afghanistan   84. 137138   AFG       Albania   100. 000000   ALB       Algeria   99. 439568   DZA       Andorra   100. 000000   AND       Angola   40. 520607   AGO     Now, we can join the two data sources       merged_df_2016 = gpd. GeoDataFrame(pd. merge(gdf, df_2016, left_on=&#39;ADM0_A3&#39;, right_on=&#39;Code&#39;))          merged_df_2016. head()           ADM0_A3   geometry   2016   Code         0   IDN   (POLYGON ((117. 7036079039552 4. 163414542001791. . .    97. 620000   IDN       1   MYS   (POLYGON ((117. 7036079039552 4. 163414542001791. . .    100. 000000   MYS       2   CHL   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .    100. 000000   CHL       3   PER   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .    94. 851746   PER       4   ARG   (POLYGON ((-68. 4486097329999 -52. 3466170159999. . .    100. 000000   ARG     Finally plotting!&#182;:       # Example borrowed from http://ramiro. org/notebook/geopandas-choropleth/cmap=&#39;OrRd&#39;figsize = (16, 5)ax = merged_df_2016. plot(column=&#39;2016&#39;, cmap=cmap, figsize=figsize,legend=True)title = &#39;Electricity Access(% of population) in {}&#39;. format(&#39;2016&#39;)gdf[~gdf. ADM0_A3. isin(merged_df_2016. ADM0_A3)]. plot(ax=ax, color=&#39;#fffafa&#39;, hatch=&#39;///&#39;)ax. set_title(title, fontdict={&#39;fontsize&#39;: 15}, loc=&#39;left&#39;)ax. set_axis_off()    Creating animation for access across time&#182;:       !mkdir -p elec_access          def save_png_year(year, path=&quot;elec_access&quot;):  df_year = df_elec. unstack()[[&#39;{}&#39;. format(year)]]. dropna()  codes = [countries. get(country, &#39;Unknown code&#39;) for country in df_year. index]  df_year[&#39;Code&#39;] = codes  merged_df_year = gpd. GeoDataFrame(pd. merge(gdf, df_year, left_on=&#39;ADM0_A3&#39;, right_on=&#39;Code&#39;))  figsize = (16, 5)  ax = merged_df_year. plot(column=&#39;{}&#39;. format(year), cmap=cmap, figsize=figsize,legend=True,vmin=0. 0, vmax=100. 0)  title = &#39;Electricity Access(% of population) in {}&#39;. format(year)  gdf[~gdf. ADM0_A3. isin(merged_df_year. ADM0_A3)]. plot(ax=ax, color=&#39;#fffafa&#39;, hatch=&#39;///&#39;)  ax. set_title(title, fontdict={&#39;fontsize&#39;: 15}, loc=&#39;left&#39;)  ax. set_axis_off()  plt. savefig(&#39;{}/{}. png&#39;. format(path, year), dpi=300)  plt. close()          for year in range(1990, 2017):  save_png_year(year)          # Borrowed from http://www. kevinwampler. com/blog/2016/09/10/creating-animated-gifs-using-python. htmldef create_gifv(input_files, output_base_name, fps):  import imageio  output_extensions = [&quot;gif&quot;]  input_filenames = [&#39;elec_access/{}. png&#39;. format(year) for year in range(1990, 2017)]  poster_writer = imageio. get_writer(&quot;{}. png&quot;. format(output_base_name), mode=&#39;i&#39;)  video_writers = [    imageio. get_writer(&quot;{}. {}&quot;. format(output_base_name, ext), mode=&#39;I&#39;, fps=fps)    for ext in output_extensions]  is_first = True  for filename in input_filenames:    img = imageio. imread(filename)    for writer in video_writers:      writer. append_data(img)    if is_first:      poster_writer. append_data(img)    is_first = False  for writer in video_writers + [poster_writer]:    writer. close()          create_gifv(&quot;elec_access/*. png&quot;, &quot;electricity_access&quot;, 4)     Across Africa and SE Asia, one can clearly see a gradual improvement in access! Hope you had fun reading this post :) "
    }, {
    "id": 9,
    "url": "https://nipunbatra.github.io/air%20quality/2018/06/21/aq-india-map.html",
    "title": "Mapping location of air quality sensing in India",
    "body": "2018/06/21 -           In this notebook, I'll show a quick example of how to use Folium (which internally uses LeafletJS) for visualising the location of air quality monitors in India. The purpose of this notebook is eductional in nature. Standard Imports&#182;:       import numpy as npimport matplotlib. pyplot as pltimport pandas as pd%matplotlib inline    Downloading data from OpenAQ for 2018-04-06&#182;:       !wget --no-check-certificate https://openaq-data. s3. amazonaws. com/2018-04-06. csv -P /Users/nipun/Downloads/  --2020-02-29 17:52:50-- https://openaq-data. s3. amazonaws. com/2018-04-06. csvResolving openaq-data. s3. amazonaws. com (openaq-data. s3. amazonaws. com). . . 52. 216. 99. 123Connecting to openaq-data. s3. amazonaws. com (openaq-data. s3. amazonaws. com)|52. 216. 99. 123|:443. . . connected. WARNING: cannot verify openaq-data. s3. amazonaws. com&#39;s certificate, issued by ‘CN=DigiCert Baltimore CA-2 G2,OU=www. digicert. com,O=DigiCert Inc,C=US’: Unable to locally verify the issuer&#39;s authority. HTTP request sent, awaiting response. . . 200 OKLength: 133839107 (128M) [text/csv]Saving to: ‘/Users/nipun/Downloads/2018-04-06. csv. 1’2018-04-06. csv. 1   37%[======&gt;       ] 47. 37M 3. 79MB/s  eta 40s  ^C        import pandas as pddf = pd. read_csv(&quot;/Users/nipun/Downloads/2018-04-06. csv&quot;)df = df[(df. country==&#39;IN&#39;)&amp;(df. parameter==&#39;pm25&#39;)]. dropna(). groupby(&quot;location&quot;). mean()          df           value   latitude   longitude       location                  Adarsh Nagar, Jaipur - RSPCB   79. 916667   26. 902909   75. 836853       Anand Kala Kshetram, Rajamahendravaram - APPCB   42. 750000   16. 987287   81. 736318       Ardhali Bazar, Varanasi - UPPCB   103. 666667   25. 350599   82. 908307       Asanol Court Area, Asanol - WBPCB   56. 833333   23. 685297   86. 945968       Ashok Nagar, Udaipur - RSPCB   114. 750000   24. 588617   73. 632140       . . .    . . .    . . .    . . .        Vasundhara, Ghaziabad, UP - UPPCB   223. 333333   28. 660335   77. 357256       Vikas Sadan, Gurgaon, Haryana - HSPCB   280. 250000   28. 450124   77. 026305       Vindhyachal STPS, Singrauli - MPPCB   144. 000000   24. 108970   82. 645580       Ward-32 Bapupara, Siliguri - WBPCB   195. 000000   26. 688305   88. 412668       Zoo Park, Hyderabad - TSPCB   82. 500000   17. 349694   78. 451437   79 rows × 3 columns   Downloading World GeoJson file&#182;:       !wget --no-check-certificate https://raw. githubusercontent. com/python-visualization/folium/master/examples/data/world-countries. json  --2020-02-29 17:53:17-- https://raw. githubusercontent. com/python-visualization/folium/master/examples/data/world-countries. jsonResolving raw. githubusercontent. com (raw. githubusercontent. com). . . 151. 101. 8. 133Connecting to raw. githubusercontent. com (raw. githubusercontent. com)|151. 101. 8. 133|:443. . . connected. WARNING: cannot verify raw. githubusercontent. com&#39;s certificate, issued by ‘CN=DigiCert SHA2 High Assurance Server CA,OU=www. digicert. com,O=DigiCert Inc,C=US’: Unable to locally verify the issuer&#39;s authority. HTTP request sent, awaiting response. . . 200 OKLength: 252515 (247K) [text/plain]Saving to: ‘world-countries. json’world-countries. jso 100%[===================&gt;] 246. 60K  376KB/s  in 0. 7s  2020-02-29 17:53:19 (376 KB/s) - ‘world-countries. json’ saved [252515/252515]  Creating india. json correspdonding to Indian data&#182;:       import jsone = json. load(open(&#39;world-countries. json&#39;,&#39;r&#39;))json. dump(e[&#39;features&#39;][73], open(&#39;india. json&#39;,&#39;w&#39;))          import foliumfolium_map = folium. Map(width = &#39;60%&#39;,height=800,location=[20, 77],            zoom_start=5,            tiles=&quot;Stamen Terrain&quot;,min_lat=7, max_lat=35, min_lon=73, max_lon=90)for x in df. iterrows():  name = x[0]  lat, lon = x[1][&#39;latitude&#39;], x[1][&#39;longitude&#39;]  folium. CircleMarker([lat, lon], radius=5, color=&#39;#000000&#39;,fill_color=&#39;#D3D3D3&#39; , fill_opacity=1). add_to(folium_map)folium. GeoJson(&#39;india. json&#39;). add_to(folium_map)  &lt;folium. features. GeoJson at 0x11e497bd0&gt;        folium_map. save(&quot;map. html&quot;)     There you go! "
    }, {
    "id": 10,
    "url": "https://nipunbatra.github.io/ml/2018/06/16/active-committee.html",
    "title": "Active Learning",
    "body": "2018/06/16 -           We all get it - AI is the new electricity. Deep neural nets are everywhere around us. But you know what, getting labelled training data can still be a big issue in many domains. This is where active learning comes in - given that we only have a small amount of labelled data, do we randomly get labels for other samples, or can we create a smarter strategy for the same? Active learning deals with the latter. Various strategies for active learning have been proposed in the past. In this post, I'll work out a trivial example of what is called query by committee. The key idea is that we create a committee of learners and choose to acquire labels for the unlabelled points for which there is maximum disaggrement amongst the committee. I'd recommend the new readers to go through this survey. In this particular post, I'd be looking at active learning via query by committee, where the committee members are trained on different subsets of the train data. In a future post, I'll write about active learning via query by committee, where the committee members are trained on the same data, but with different parameters. Standard imports&#182;:       import numpy as npimport matplotlib. pyplot as pltimport pandas as pdimport warningswarnings. filterwarnings(&#39;ignore&#39;)np. random. seed(0)%matplotlib inline    Creating dataset&#182;:       X = np. arange(1, 1001, 1)Y = 10*X + 4 + 400* np. random. randn(1000, )           plt. scatter(X, Y, s=0. 1)plt. xlabel(&quot;X&quot;)plt. ylabel(&quot;Y&quot;)  Text(0, 0. 5, &#39;Y&#39;)  Learning a linear regression model on the entire data&#182;:       from sklearn. linear_model import LinearRegressionclf = LinearRegression()          clf. fit(X. reshape(-1,1), Y)  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)        clf. intercept_  -10. 370897712972692        clf. coef_  array([9. 99254389])  Visualising the fit&#182;:       plt. scatter(X, Y, s=0. 1)plt. xlabel(&quot;X&quot;)plt. ylabel(&quot;Y&quot;)plt. plot(X, clf. coef_[0]*X + clf. intercept_, color=&#39;k&#39;, label=&#39;Best fit on all data&#39;)plt. legend()plt. text(500, clf. coef_[0]*500 + clf. intercept_ +4000, &quot;Y = {0:0. 2f} X + {1:0. 2f}&quot;. format(clf. coef_[0], clf. intercept_) )  Text(500, 8985. 90104506115, &#39;Y = 9. 99 X + -10. 37&#39;)  Creating the initial train set, the test set and the pool&#182;:       from sklearn. model_selection import train_test_split          train_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0. 5)          train_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, test_size=495)          plt. scatter(train_X, train_Y)  &lt;matplotlib. collections. PathCollection at 0x1a211e9150&gt;  Creating a committee each learnt on different subset of the data&#182;:       committee_size = 5          train_X_com = {0:{}}train_Y_com = {0:{}}models_com = {0:{}}iteration = 0for cur_committee in range(committee_size):  train_X_com[iteration][cur_committee], _, train_Y_com[iteration][cur_committee], _ = train_test_split(train_X, train_Y, train_size=0. 5,                                        random_state=cur_committee)  models_com[iteration][cur_committee] = LinearRegression()  models_com[iteration][cur_committee]. fit(train_X_com[iteration][cur_committee]. reshape(-1,1), train_Y_com[iteration][cur_committee])    Plotting the fit of the committee on the entire dataset&#182;:       plt. scatter(X, Y, s=0. 2)for cur_committee in range(committee_size):  plt. plot(X, models_com[0][cur_committee]. coef_[0]*X + models_com[0][cur_committee]. intercept_,       label=&#39;Model {0}\nY = {1:0. 2f} X + {2:0. 2f}&#39;. format(cur_committee,                                 models_com[0][cur_committee]. coef_[0],                                models_com[0][cur_committee]. intercept_))  plt. legend()    Evaluate the performance on the test set&#182;:       estimations_com = {0:{}}for cur_committee in range(committee_size):  estimations_com[0][cur_committee] = models_com[0][cur_committee]. predict(test_X. reshape(-1, 1))          test_mae_error = {0:(pd. DataFrame(estimations_com[0]). mean(axis=1) - test_Y). abs(). mean()}    The MAE on the test set is:       test_mae_error[0]  565. 8837967341798  Active learning procedure&#182;:       num_iterations = 20points_added_x=[]points_added_y=[]print(&quot;Iteration, Cost\n&quot;)print(&quot;-&quot;*40)for iteration in range(1, num_iterations):  # For each committee: making predictions on the pool set based on model learnt in the respective train set   estimations_pool = {cur_committee: models_com[iteration-1][cur_committee]. predict(pool_X. reshape(-1, 1)) for cur_committee in range(committee_size)}  # Finding points from the pool with highest disagreement among the committee - highest standard deviation  in_var = pd. DataFrame(estimations_pool). std(axis=1). argmax()    to_add_x = pool_X[in_var]  to_add_y = pool_Y[in_var]  points_added_x. append(to_add_x)  points_added_y. append(to_add_y)    # For each committee - Adding the point where the committe most disagrees  for com in range(committee_size):    if iteration not in train_X_com:      train_X_com[iteration] = {}      train_Y_com[iteration] = {}      models_com[iteration] = {}    train_X_com[iteration][com] = np. append(train_X_com[iteration-1][com], to_add_x)    train_Y_com[iteration][com] = np. append(train_Y_com[iteration-1][com], to_add_y)    # Deleting the point from the pool  pool_X = np. delete(pool_X, in_var)  pool_Y = np. delete(pool_Y, in_var)    # Training on the new set for each committee  for cur_committee in range(committee_size):    models_com[iteration][cur_committee] = LinearRegression()    models_com[iteration][cur_committee]. fit(train_X_com[iteration][cur_committee]. reshape(-1,1), train_Y_com[iteration][cur_committee])    estimations_com[iteration] = {}  for cur_committee in range(committee_size):    estimations_com[iteration][cur_committee] = models_com[iteration][cur_committee]. predict(test_X. reshape(-1, 1))  test_mae_error[iteration]=(pd. DataFrame(estimations_com[iteration]). mean(axis=1) - test_Y). abs(). mean()  print(iteration, (test_mae_error[iteration]))  Iteration, Cost----------------------------------------1 406. 176648980548752 402. 98977527159863 348. 451827390542354 348. 495195150399075 349. 041979384757166 348. 681885778048077 352. 408826685732668 373. 604172082798649 377. 2504457170572310 372. 530214304521611 335. 3024305611560312 336. 607360666066613 343. 286783799892314 347. 049126637330615 349. 746419527443616 351. 599083363103917 349. 2195754803497618 338. 876522320647619 337. 0132510959355        pd. Series(test_mae_error). plot(style=&#39;ko-&#39;)plt. xlim((-0. 5, num_iterations+0. 5))plt. ylabel(&quot;MAE on test set&quot;)plt. xlabel(&quot;# Points Queried&quot;)  Text(0. 5, 0, &#39;# Points Queried&#39;)  As expected, the error goes down as we increase the number of points queried       fig, ax = plt. subplots()import osfrom matplotlib. animation import FuncAnimationplt. rcParams[&#39;animation. ffmpeg_path&#39;] = os. path. expanduser(&#39;/Users/nipun/ffmpeg&#39;)def update(iteration):  ax. cla()  ax. scatter(X, Y, s=0. 2)  ax. set_title(&quot;Iteration: {} \n MAE = {:0. 2f}&quot;. format(iteration, test_mae_error[iteration]))  for cur_committee in range(committee_size):    ax. plot(X, models_com[iteration][cur_committee]. coef_[0]*X + models_com[iteration][cur_committee]. intercept_,       label=&#39;Model {0}\nY = {1:0. 2f} X + {2:0. 2f}&#39;. format(cur_committee,                                 models_com[iteration][cur_committee]. coef_[0],                                models_com[iteration][cur_committee]. intercept_))        ax. scatter(points_added_x[iteration], points_added_y[iteration],s=100, color=&#39;red&#39;)  ax. legend()    fig. tight_layout()anim = FuncAnimation(fig, update, frames=np. arange(0, num_iterations-1, 1), interval=1000)plt. close()          from IPython. display import HTMLHTML(anim. to_html5_video())    Your browser does not support the video tag.   From the animation, we can see that how adding a new point to the train set (shown in red) reduces the variation in prediction amongst the different committee members. "
    }, {
    "id": 11,
    "url": "https://nipunbatra.github.io/ml/2018/01/13/denoising.html",
    "title": "Signal denoising using RNNs in PyTorch",
    "body": "2018/01/13 -           In this post, I'll use PyTorch to create a simple Recurrent Neural Network (RNN) for denoising a signal. I started learning RNNs using PyTorch. However, I felt that many of the examples were fairly complex. So, here's an attempt to create a simple educational example. Problem description&#182;: Given a noisy sine wave as an input, we want to estimate the denoised signal. This is shown in the figure below.  Customary imports&#182;:       import numpy as npimport math, randomimport matplotlib. pyplot as plt%matplotlib inlinenp. random. seed(0)    Creating noisy and denoised signals&#182;: Let's now write functions to cerate a sine wave, add some noise on top of it. This way we're able to create a noisy verison of the sine wave.       # Generating a clean sine wave def sine(X, signal_freq=60. ):  return np. sin(2 * np. pi * (X) / signal_freq)# Adding uniform noisedef noisy(Y, noise_range=(-0. 35, 0. 35)):  noise = np. random. uniform(noise_range[0], noise_range[1], size=Y. shape)  return Y + noise# Create a noisy and clean sine wave def sample(sample_size):  random_offset = random. randint(0, sample_size)  X = np. arange(sample_size)  out = sine(X + random_offset)  inp = noisy(out)  return inp, out    Let's now invoke the functions we defined to generate the figure we saw in the problem description.       inp, out = sample(100)plt. plot(inp, label=&#39;Noisy&#39;)plt. plot(out, label =&#39;Denoised&#39;)plt. legend()  &lt;matplotlib. legend. Legend at 0x106beb828&gt;  Creating dataset&#182;: Now, let's write a simple function to generate a dataset of such noisy and denoised samples.       def create_dataset(n_samples=10000, sample_size=100):  data_inp = np. zeros((n_samples, sample_size))  data_out = np. zeros((n_samples, sample_size))    for i in range(n_samples):    sample_inp, sample_out = sample(sample_size)    data_inp[i, :] = sample_inp    data_out[i, :] = sample_out  return data_inp, data_out    Now, creating the dataset, and dividing it into train and test set.       data_inp, data_out = create_dataset()train_inp, train_out = data_inp[:8000], data_out[:8000]test_inp, test_out = data_inp[8000:], data_out[8000:]          import torchimport torch. nn as nnfrom torch. autograd import Variable    Creating RNN&#182;: We have 1d sine waves, which we want to denoise. Thus, we have input dimension of 1. Let's create a simple 1-layer RNN with 30 hidden units.       input_dim = 1hidden_size = 30num_layers = 1class CustomRNN(nn. Module):  def __init__(self, input_size, hidden_size, output_size):    super(CustomRNN, self). __init__()    self. rnn = nn. RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)    self. linear = nn. Linear(hidden_size, output_size, )    self. act = nn. Tanh()  def forward(self, x):    pred, hidden = self. rnn(x, None)    pred = self. act(self. linear(pred)). view(pred. data. shape[0], -1, 1)    return predr= CustomRNN(input_dim, hidden_size, 1)          r  CustomRNN ( (rnn): RNN(1, 30, batch_first=True) (linear): Linear (30 -&gt; 1) (act): Tanh ())  Training&#182;:       # Storing predictions per iterations to visualise laterpredictions = []optimizer = torch. optim. Adam(r. parameters(), lr=1e-2)loss_func = nn. L1Loss()for t in range(301):  hidden = None  inp = Variable(torch. Tensor(train_inp. reshape((train_inp. shape[0], -1, 1))), requires_grad=True)  out = Variable(torch. Tensor(train_out. reshape((train_out. shape[0], -1, 1))) )  pred = r(inp)  optimizer. zero_grad()  predictions. append(pred. data. numpy())  loss = loss_func(pred, out)  if t%20==0:    print(t, loss. data[0])  loss. backward()  optimizer. step()  0 0. 577493071556091320 0. 1202814728021621740 0. 1125186309218406760 0. 1083483397960662880 0. 11243857443332672100 0. 11533079296350479120 0. 09951132535934448140 0. 078636534512043160 0. 08674494177103043180 0. 07217984646558762200 0. 06266186386346817220 0. 05793667957186699240 0. 0723448321223259260 0. 05628745257854462280 0. 050240203738212585300 0. 06297950446605682  Great. As expected, the loss reduces over time. Generating prediction on test set&#182;:       t_inp = Variable(torch. Tensor(test_inp. reshape((test_inp. shape[0], -1, 1))), requires_grad=True)pred_t = r(t_inp)          # Test lossprint(loss_func(pred_t, Variable(torch. Tensor(test_out. reshape((test_inp. shape[0], -1, 1))))). data[0])  0. 06105425953865051  Visualising sample denoising&#182;:       sample_num = 23plt. plot(pred_t[sample_num]. data. numpy(), label=&#39;Pred&#39;)plt. plot(test_out[sample_num], label=&#39;GT&#39;)plt. legend()plt. title(&quot;Sample num: {}&quot;. format(sample_num))  &lt;matplotlib. text. Text at 0x1064675c0&gt;  Bidirectional RNN&#182;: Seems reasonably neat to me! If only the first few points were better esimtated. Any idea why they're not? Maybe, we need a bidirectional RNN? Let's try one, and I'll also add dropout to prevent overfitting.       bidirectional = Trueif bidirectional:  num_directions = 2else:  num_directions = 1class CustomRNN(nn. Module):  def __init__(self, input_size, hidden_size, output_size):    super(CustomRNN, self). __init__()    self. rnn = nn. RNN(input_size=input_size, hidden_size=hidden_size,              batch_first=True, bidirectional=bidirectional, dropout=0. 1)    self. linear = nn. Linear(hidden_size*num_directions, output_size, )    self. act = nn. Tanh()  def forward(self, x):    pred, hidden = self. rnn(x, None)    pred = self. act(self. linear(pred)). view(pred. data. shape[0], -1, 1)    return predr= CustomRNN(input_dim, hidden_size, 1)r  CustomRNN ( (rnn): RNN(1, 30, batch_first=True, dropout=0. 1, bidirectional=True) (linear): Linear (60 -&gt; 1) (act): Tanh ())        # Storing predictions per iterations to visualise laterpredictions = []optimizer = torch. optim. Adam(r. parameters(), lr=1e-2)loss_func = nn. L1Loss()for t in range(301):  hidden = None  inp = Variable(torch. Tensor(train_inp. reshape((train_inp. shape[0], -1, 1))), requires_grad=True)  out = Variable(torch. Tensor(train_out. reshape((train_out. shape[0], -1, 1))) )  pred = r(inp)  optimizer. zero_grad()  predictions. append(pred. data. numpy())  loss = loss_func(pred, out)  if t%20==0:    print(t, loss. data[0])  loss. backward()  optimizer. step()  0 0. 682519912719726620 0. 1110497191548347540 0. 0773264169692993260 0. 0721015259623527580 0. 06964801251888275100 0. 06717491149902344120 0. 06266810745000839140 0. 06302479654550552160 0. 05954732000827789180 0. 05402040109038353200 0. 05266999825835228220 0. 06145058199763298240 0. 0500367134809494260 0. 05388529226183891280 0. 053044941276311874300 0. 046826526522636414        t_inp = Variable(torch. Tensor(test_inp. reshape((test_inp. shape[0], -1, 1))), requires_grad=True)pred_t = r(t_inp)          # Test lossprint(loss_func(pred_t, Variable(torch. Tensor(test_out. reshape((test_inp. shape[0], -1, 1))))). data[0])  0. 050666142255067825        sample_num = 23plt. plot(pred_t[sample_num]. data. numpy(), label=&#39;Pred&#39;)plt. plot(test_out[sample_num], label=&#39;GT&#39;)plt. legend()plt. title(&quot;Sample num: {}&quot;. format(sample_num))  &lt;matplotlib. text. Text at 0x126f22710&gt;  Hmm. The estimated signal looks better for the initial few points. But, gets worse for the final few points. Oops! Guess, now the reverse RNN causes problems for its first few points! From RNNs to GRU&#182;: Let's now replace our RNN with GRU to see if the model improves.       bidirectional = Trueif bidirectional:  num_directions = 2else:  num_directions = 1class CustomRNN(nn. Module):  def __init__(self, input_size, hidden_size, output_size):    super(CustomRNN, self). __init__()    self. rnn = nn. GRU(input_size=input_size, hidden_size=hidden_size,              batch_first=True, bidirectional=bidirectional, dropout=0. 1)    self. linear = nn. Linear(hidden_size*num_directions, output_size, )    self. act = nn. Tanh()  def forward(self, x):    pred, hidden = self. rnn(x, None)    pred = self. act(self. linear(pred)). view(pred. data. shape[0], -1, 1)    return predr= CustomRNN(input_dim, hidden_size, 1)r  CustomRNN ( (rnn): GRU(1, 30, batch_first=True, dropout=0. 1, bidirectional=True) (linear): Linear (60 -&gt; 1) (act): Tanh ())        # Storing predictions per iterations to visualise laterpredictions = []optimizer = torch. optim. Adam(r. parameters(), lr=1e-2)loss_func = nn. L1Loss()for t in range(201):  hidden = None  inp = Variable(torch. Tensor(train_inp. reshape((train_inp. shape[0], -1, 1))), requires_grad=True)  out = Variable(torch. Tensor(train_out. reshape((train_out. shape[0], -1, 1))) )  pred = r(inp)  optimizer. zero_grad()  predictions. append(pred. data. numpy())  loss = loss_func(pred, out)  if t%20==0:    print(t, loss. data[0])  loss. backward()  optimizer. step()  0 0. 629428148269653320 0. 1145239472389221240 0. 0854871943593025260 0. 0710101574659347580 0. 05964939296245575100 0. 053830236196517944120 0. 06312716007232666140 0. 04494623467326164160 0. 04309168830513954180 0. 04010637104511261200 0. 035212572664022446        t_inp = Variable(torch. Tensor(test_inp. reshape((test_inp. shape[0], -1, 1))), requires_grad=True)pred_t = r(t_inp)          # Test lossprint(loss_func(pred_t, Variable(torch. Tensor(test_out. reshape((test_inp. shape[0], -1, 1))))). data[0])  0. 03618593513965607        sample_num = 23plt. plot(pred_t[sample_num]. data. numpy(), label=&#39;Pred&#39;)plt. plot(test_out[sample_num], label=&#39;GT&#39;)plt. legend()plt. title(&quot;Sample num: {}&quot;. format(sample_num))  &lt;matplotlib. text. Text at 0x11661e208&gt;  The GRU prediction seems to far better! Maybe, the RNNs suffer from the vanishing gradients problem? Visualising estimations as model improves&#182;: Let's now write a simple function to visualise the estimations as a function of iterations. We'd expect the estimations to improve over time.       plt. rcParams[&#39;animation. ffmpeg_path&#39;] = &#39;. /ffmpeg&#39;from matplotlib. animation import FuncAnimationfig, ax = plt. subplots(figsize=(4, 3))fig. set_tight_layout(True)# Query the figure&#39;s on-screen size and DPI. Note that when saving the figure to# a file, we need to provide a DPI for that separately. print(&#39;fig size: {0} DPI, size in inches {1}&#39;. format(  fig. get_dpi(), fig. get_size_inches()))def update(i):  label = &#39;Iteration {0}&#39;. format(i)  ax. cla()  ax. plot(np. array(predictions)[i, 0, :, 0]. T, label=&#39;Pred&#39;)  ax. plot(train_out[0, :], label=&#39;GT&#39;)  ax. legend()  ax. set_title(label) anim = FuncAnimation(fig, update, frames=range(0, 201, 4), interval=20)anim. save(&#39;learning. mp4&#39;,fps=20)plt. close()  fig size: 72. 0 DPI, size in inches [ 4.  3. ]        from IPython. display import VideoVideo(&quot;learning. mp4&quot;)     Your browser does not support the video element.     This looks great! We can see how our model learns to learn reasonably good denoised signals over time. It doesn't start great though. Would a better initialisation help? I certainly feel that for this particular problem it would, as predicting the output the same as input is a good starting point! Bonus: Handling missing values in denoised training data&#182;: The trick to handling missing values in the denoised training data (the quantity we wish to estimate) is to compute the loss only over the present values. This requires creating a mask for finding all entries except missing. One such way to do so would be: mask = out &gt; -1* 1e8 where out is the tensor containing missing values. Let's first add some unknown values (np. NaN) in the training output data.       for num_unknown_values in range(50):  train_out[np. random. choice(list(range(0, 8000))), np. random. choice(list(range(0, 100)))] = np. NAN          np. isnan(train_out). sum()  50  Testing using a network with few parameters.       r= CustomRNN(input_dim, 2, 1)r  CustomRNN ( (rnn): GRU(1, 30, batch_first=True, dropout=0. 1, bidirectional=True) (linear): Linear (60 -&gt; 1) (act): Tanh ())        # Storing predictions per iterations to visualise laterpredictions = []optimizer = torch. optim. Adam(r. parameters(), lr=1e-2)loss_func = nn. L1Loss()for t in range(20):  hidden = None  inp = Variable(torch. Tensor(train_inp. reshape((train_inp. shape[0], -1, 1))), requires_grad=True)  out = Variable(torch. Tensor(train_out. reshape((train_out. shape[0], -1, 1))) )  pred = r(inp)  optimizer. zero_grad()  predictions. append(pred. data. numpy())  # Create a mask to compute loss only on defined quantities  mask = out &gt; -1* 1e8  loss = loss_func(pred[mask], out[mask])  if t%20==0:    print(t, loss. data[0])  loss. backward()  optimizer. step()  0 0. 6575785279273987  There you go! We've also learnt how to handle missing values! I must thank Simon Wang and his helpful inputs on the PyTorch discussion forum. "
    }, {
    "id": 12,
    "url": "https://nipunbatra.github.io/academia/2018/01/07/cs-phd-lessons.html",
    "title": "CS Ph.D. lessons to my younger self",
    "body": "2018/01/07 -           I completed my CS Ph. D. from IIIT Delhi (Note the three Is) in March 2017. My Ph. D. was a lesson filled journey. Here’s a letter from me to my younger self for CS Ph. D. lessons. Dear younger self, As you embark this Ph. D. journey, I wanted to share some lessons and experiences. While I do not claim to be able to follow this 100 %, I am more aware than I was before. I think I might have done a better job at my Ph. D. if I'd taken note of the points I am going to mention now (in no particular order). Take care of your health&#182;: You're joining fresh after completing your undergraduate studies. You are full of energy. But, your body won't remain the same. You'll realise that there is a difference between the early and late 20s. You'll be debugging your code, and will feel it'll take only 5 minutes more, and after three hours realise you're well past your routine, your neck may hurt! Very soon, your body might hunch forward because of long sitting hours staring at your screen, and you may start gaining weight. Remember that when optimising for a happy life, you can't neglect your health for long. As one of my mentors would say, take time off. Set it on your calendar! Ensure that you get some form of exercise routinely. By health, I mean not only your physical health but also your mental health. There'll be times that you'd be exhausted and dejected. No matter what you do, you don't seem to make any progress. You'll start developing doubts about your abilities. You'll start questioning your decision to pursue a Ph. D. Remember that you're not alone! Taking time off would help. It would also greatly help to have friends to talk and share. Don't keep all your anxieties and worries to yourself! Meet well!&#182;: You'd be participating in a lot of meetings over the tenure of your Ph. D. Most often you'd be meeting your research advisor. You'd be very excited to discuss every possible detail of your work with your advisor. But, remember, that your advisor probably has some courses to take. They might have a group of students working under them, and have multiple projects running simultaneously. So, you'd need to plan and strategise to make the best use of the meeting time you get with your advisor. Here's a template that I tried to follow. First, set the context by giving a summary of previous meeting discussion. Setting the context will ensure that you both are on the same page. Present a brief agenda. Then, present the main findings. Have some thoughts on why your method works or does not work. Over the course of your Ph. D. , you'll realise that your advisor is an expert in deducing why something works, and why something doesn't. It'll be precious for you to observe your advisor and learn and apply this yourself. In my earlier years, I'd often just go and show all my results to my advisor. You really shouldn't stop there. Think about what you should conclude from your findings. Come up with specific questions for your advisor. Ensure that you make the best use of the limited meeting time you get with your advisor. Paper rejection is tough, but not the end of the world&#182;: It's likely that you'll have a few papers rejected during your Ph. D. I couldn't believe when I got my first paper rejected. How could it be! In anguish, I felt the reviewer didn't do a good job. I took it very personally! With more rejections, I got somewhat better at it. I also learned lessons along the way. One of the lessons that stuck with me was to not ponder too much on the reviews for 1-2 days after the paper was rejected. I wasn't able to take the reviews with an unbiased attitude anyway. After 1-2 days, I would often appreciate the points made by the reviewers. Sometimes, I wouldn't, but, then reviewing is complex, anyway! It should also be noted that of all my total submissions, only about one-third or so got accepted. Top CS conferences are very competitive, so paper rejections are likely to be more common than not! Better emails go a long way&#182;: You'd be writing a ton of emails during your Ph. D. , way more often than meeting in person. It's important that you learn the art and science of writing better emails. Poorly written emails are less likely to get a response. At the beginning of my Ph. D. , I wrote poor emails. The subject line wasn't indicative of what to expect in the post, which made it incredibly hard to find relevant threads later! I often jumbled up multiple projects/topics in one email. I often wrote very long emails. Many of my emails were a brain dump and not organised well. Eventually, by observing how my advisor and other senior people wrote emails, I improved in this important skill. I think that many of the points about meetings are also applicable to emails. You need to set the context, discuss how you proceeded, and why you did so, summarise the observation, form some conclusions and questions, and if possible be specific on what inputs you need. Embrace the scientific method&#182;: I was an engineer by training before I started my Ph. D. So, I'd be thinking like -  this seems cool. Let's try this approach. Should be fun to solve . I went this way for some time, till I had a meeting with a senior faculty. The first question he asked me was -  what's your hypothesis? . I stood dumbfounded. I realised that thus far I'd been a solution-first researcher without doing so in a scientific way. I'd try and pick up approaches, if they didn't work, move on. After a bit of rewiring, I improved my understanding and application of the scientific method. So, my work would now be more structured. I'd be wasting less time on random trails or unimportant problems. The key lesson is to have a checklist (mental or physical) of common pitfalls - like, improving the accuracy of a system from 95% to 99% on an unimportant or extremely contrived problem; or, starting data collection without much thought to the final experiments and analysis you'd plan to conduct to prove your hypothesis; or, the worst of all, having no hypothesis to start off with! Mid-PhD. crisis time: you're not alone&#182;: I'd heard that every Ph. D. student goes through some crisis period. I thought I'd be an exception. How wrong I was. This period occurred after I'd been academically weaned. My coursework had finished. There was little barring research to keep me busy. Paper rejections stopped surprising me. Ideas not working stopped surprising me. I started questioning if I should quit Ph. D. midway. I started looking at forums on this topic and realised that this problem was common. During this period, I kept in touch with folks who'd completed their PhDs. Everyone suggested that this is normal, and a Ph. D. wouldn't be so valuable if it weren't this difficult! I feel that staying in touch with people who could relate to me was important. It was also helpful that despite the so-called inputs not converting to output, my advisors continued encouraging me, meeting regularly, and discussing scientifically correct ways of finding solutions to the Ph. D. problem. Miraculously this phase ended and led to the most successful phase of my Ph. D. where I was able to submit and get accepted a few top-tier conference papers. The key lesson is to stick it out, don't feel alone or worthless, keep talking to cheerful people and keep reporting progress to your advisor on a regular basis. It all boils down to addition and subtraction: Blog posts&#182;: I won't lie, I often get intimidated by research papers and the huge amount of techniques in our field. Yes, I still do. One of best teachers at my university spoke -  it all boils down to addition and subtraction.   This has stuck with me. I took a programming and visualisation approach to understanding concepts in my field. At a higher level, I'd be thinking that if I had to teach this concept to someone, how would I go about it. For example, if say, I wanted to study dynamic time warping, I started with some trivial problems where I'd use the concept. On such trivial problems, it would be easy to understand what the algorithm would be doing. I'd often end up writing detailed Jupyter/IPython notebooks showing how the algorithm works, programming and visualsing the various steps. All these formed a part of my technical blog, which I would update on a regular basis. The learning in writing these blog posts was immense. While these blog posts are public, I think I am the biggest beneficiary. Not only does one gain a good understanding of the concept involved, but one also gains confidence about the subject and one's ability to understand! The key lesson is to document your learnings, understandings, and try to abstract out your specific problem and think of teaching the concept to someone who doesn't know much about your problem. Teaching is learning&#182;: A teaching assistantship is often dreaded. Why waste my valuable time on it? It turns out, I learned a lot from my TA experiences. I realised that learning with an intention to share the learning ensured that I learned well. I didn't cut corners. Moreover, with an emphasis on teaching well, I often had to think hard about making concepts relatable. This exercise helped me a lot! In my first semester, I was a TA for an introduction to programming course. Before the course, I thought I knew Python reasonably well. After the course, I realised that now I knew it way better than earlier. Since Python turned out to be the language I did most of my research coding in, it turned out to be a great learning experience. Besides, I made a lot of friends with my students in the course! The key lesson here is somewhat related to the lesson on blog posts. Thinking how to explain stuff usually always helped me get a better understanding! Good research is only half done!&#182;: Talks are a great way to advertise your research and get some good feedback. It's easy to go very excited and fit everything from the paper into a few slides in a 15-20 minute presentation. This is a great recipe for disaster! Good presentations often leave the audience feeling thankful that they attended the talk. Bad talks ensure that people open their laptops and start answering emails they've not replied for ages! A good talk requires preparation. I repeat. A good talk requires preparation. Even if you're a pro. Over the course of the years, I developed my style learning from my advisors, other faculties, other presenters. There were a lot of lessons involved! One of the key lessons for me was to jot down a script for my slides. While I felt I could mostly do a good job speaking without speaker notes, I think I was better with them! Of course, it took a lot of effort! Another important lesson was to deliver a talk in terms of things the audience could relate with, and thus keeping them engaged. I also maintained that the slides are there to support me and not replace me. Thus, I would never put much text into them! I'd put a lot of efforts in maintaining consistency across slides, using similar figures, conventions. All of this to ensure that the viewer doesn't lose interest! Of course, despite all these efforts, I would usually always deliver a pathetic first talk. I was lucky that most of these first pathetic talks came in front of colleagues and not the main audience. So,  Practice! Practice! And practice!  is the mantra. Volunteer to review papers&#182;: Your advisor would likely be reviewing a lot of papers throughout the year. Many of these would probably be in your sub-area of CS. It was an excellent exercise for me when I'd help my advisor review some of the papers. Taking part in the review process makes you appreciate the time and effort put in by the TPC and the conference management team! No one is paid for all this effort! I particularly remember excitedly telling my advisor that I'd champion one of the papers he gave me to review. He smiled and said that paper wouldn't probably go through. The discussion that followed helped me learn the traits of a good and a bad paper from a reviewer's perspective. Once you get the hang of it and do the same critical analysis of your paper, you'd be able to improve your paper! Open source is the right way&#182;: CS research often involves writing code for our approach, some baselines, data management and analysis. This set of code leads to the generation of results and analysis in our papers. Now, when I started my Ph. D. and started working on a problem, I thought that it should be trivial to compare our work to previous literature. It turns out; I was wrong. Reproducibility or the act of generating results from previous literature is a huge challenge. Making your code reproducible, such that others can use it for their analysis and paper takes a lot of effort. I felt that it was the right thing to make my code open source and easy for others to use. One of my most cited paper came as a result of introducing more transparency, comparability, and reproducibility. I'm also not surprised that I'm the biggest benefactor by making my code good enough to be made public. The steps I took to make the code more readable, and reproducible, helped me a lot going back to my code to tweak or re-run some experiments. In the age of Github, there aren't really many excuses for not putting efforts towards -  Let's make scientific articles comparable again.   Funding for conferences/PhD scholarships&#182;: While we may crib about our stipends not being comparable to industry folks, let's not forget that a Ph. D. can be very expensive. Coming from India, the travel costs to conferences would be high! Yes, very few of the international conferences I attended happened in India. However, some research labs like MSR and Google and some government agencies provide funding for  good  conferences. Many conferences also provide economic support. I was lucky that I could cut some of the costs for my advisor and department by getting travel fundings. Various organisations also offer Ph. D. fellowships. I was fortunate to receive one from TCS. These fellowships not only allow for some industry mentors but also include financial support. It's one less thing to worry if we can get some of the finances worked out. The key lesson isn't a particularly surprising one - apply for fellowship and grants whenever you can! Good paper writing takes time and practice&#182;: Before joining grad school, I was a great writer. After completing it, I could manage some writing. Confused? When I entered grad school did I realise how pathetic my scientific writing skills were. While I'd been speaking and writing English since I was four or so, it wasn't my first language. The red coloured paper returned by my advisor on my first draft was an eye opener (technically, it was \color in LaTeX and not a hard copy!). For someone like me who loves to learn by observation, the process of taking my first draft and seeing it turn into a well-written document courtesy my advisor was wonderful. The key lesson is to observe what makes a good draft. I came up with a structure that I would follow in almost all of my papers: What is the general problem?How can we convert it to a CS problem?What's the related work and where does it fail to address the problem?Why would our approach work? and what's the gist of it?What's the experimental setup I've used for evaluation?How well does our approach perform?In light of above, what can be conclude about the problem given our approach?I'd write using such a structure in the abstract and just expand it into different sections in the paper. Oh, and yes, don't forget Grammarly and some scripts from Matt Might! Something that really helped me was to run my drafts, even before the final version with my colleagues to get some reviews. Accept limitations of your work and be honest about them&#182;: We've all seen those few points on the graph that make our algorithm look bad. Only if we didn't have those points, our technique would look so amazing! There may be a tendency to  avoid  the limitations. My key learning on this front has been to accept the limitations of my work and be very honest about them. Science loses if we  hide  facts. Rather, it's the limitations which make things interesting. They force us to go back and review everything. Is this an issue with our approach, or implementation, or dataset, or something more fundamental? Maintaining this integrity was always a top priority! There've been instances where paper reviewers have shown appreciation for us being clear and honest about the limitations. Sometimes it helps to disconnect&#182;: We live in an age of data deluge. There are so many forums to network and brand your work. There are so many forums like Reddit and HackerNews to remain on top of the latest in the field. While I tried to remain up to date with the latest, it helped when I would sometimes disconnect. This used to be the way I studied in primary and secondary school. So, if any idea or interesting problem comes to mind, I would sometimes try and think it through before googling it. Similarly, if the code gives some error, I found that my programming and understanding improved if I would spend some time thinking before googling! The key lesson is to think before you search. Is the grass really greener on the other side&#182;: As a Ph. D. student sitting in India, I'd often think how great it would have been to be a Ph. D. student at say MIT, Stanford, or some other top CS university! Maybe, I would have a  better  publication record. Procrastinating in such situations is easy. My key learning in this aspect came from a discussion I had about 15 years back in my high school when my teacher told me that it's the students who make the school. So, the key lesson was to accept that I may not be from the most well-known school in the world, but nothing stops me from doing world-class research. So, accepting what I had, and trying to change what I could was the main learning. Of course, research is highly collaborative these days. Eventually, by the time I had graduated, I had worked with people from Imperial London, Univ. of Southampton, University of Virginia, UCLA and CMU. Invest in good hardware&#182;: I get it. Our Ph. D. stipends aren't super amazing. I chose to buy the  best  laptop I could in a reasonable budget. Why bother with Apple-like expensive laptops. I could do with a regular laptop and just install Linux on it. It turns out, I was wrong. Within two years, my laptop had a broken hinge, I had driver issues (on Linux), the laptop wasn't super light, the battery wasn't all that great. Then, it took me a hard time to move to a superior laptop. It was expensive. But, it more than made up in terms of the increased productivity. The battery would often last a work day; my back pain got better due to a lighter laptop. I was no longer afraid of updating my system. So, while I am talking about a laptop here, the lesson is more general. The cost of a  good  piece of hardware can be easily recovered many times over in terms of increased productivity. Take care! More to come&#182;: Networking at conferences, discussion buddy, elevator pitch, attending thesis and faculty job talks. . "
    }, {
    "id": 13,
    "url": "https://nipunbatra.github.io/ml/2017/12/29/neural-collaborative-filtering.html",
    "title": "Neural Networks for Collaborative Filtering",
    "body": "2017/12/29 -           Recently, I had a chance to read an interesting WWW 2017 paper entitled: Neural Collaborative Filtering. The first paragraph of the abstract reads as follows: In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback. I'd recently written a blog post on using Keras (deep learning library) for implementing traditional matrix factorization based collaborative filtering. So, I thought to get my hands dirty with building a prototype for the paper mentioned above. The authors have already provided their code on Github, which should serve as a reference for the paper and not my post, whose purpose is merely educational!Here's how the proposed network architecture looks in the paper: There are a few terms that we need to understand: User (u) and Item (i) are used to create embeddings (low-dimensional) for user and itemGeneralized Matrix Factorisation (GMF) combines the two embeddings using the dot product. This is our regular matrix factorisation. Multi-layer perceptron can also create embeddings for user and items. However, instead of taking a dot product of these to obtain the rating, we can concatenate them to create a feature vector which can be passed on to the further layers. Neural MF can then combine the predictions from MLP and GMF to obtain the following prediction. As done in my previous post, I'll use the MovieLens-100k dataset for illustration. Please refer to my previous post for more details. Peak into the dataset&#182;:       import pandas as pdimport numpy as npimport matplotlib. pyplot as pltimport warningswarnings. filterwarnings(&#39;ignore&#39;)%matplotlib inline          dataset = pd. read_csv(&quot;/Users/nipun/Downloads/ml-100k/u. data&quot;,sep=&#39;\t&#39;,names=&quot;user_id,item_id,rating,timestamp&quot;. split(&quot;,&quot;))          dataset. head()           user_id   item_id   rating   timestamp         0   196   242   3   881250949       1   186   302   3   891717742       2   22   377   1   878887116       3   244   51   2   880606923       4   166   346   1   886397596     So, each record (row) shows the rating for a user, item (movie) pair. It should be noted that I use item and movie interchangeably in this post.       len(dataset. user_id. unique()), len(dataset. item_id. unique())  (943, 1682)  We assign a unique number between (0, #users) to each user and do the same for movies.       dataset. user_id = dataset. user_id. astype(&#39;category&#39;). cat. codes. valuesdataset. item_id = dataset. item_id. astype(&#39;category&#39;). cat. codes. values          dataset. head()           user_id   item_id   rating   timestamp         0   195   241   3   881250949       1   185   301   3   891717742       2   21   376   1   878887116       3   243   50   2   880606923       4   165   345   1   886397596     Train test split&#182;: We'll now split our dataset of 100k ratings into train (containing 80k ratings) and test (containing 20k ratings). Given the train set, we'd like to accurately estimate the ratings in the test set.       from sklearn. model_selection import train_test_splittrain, test = train_test_split(dataset, test_size=0. 2)          train. head()           user_id   item_id   rating   timestamp         13185   71   95   5   880037203       23391   144   509   4   882181859       90744   895   50   2   887159951       3043   255   279   5   882151167       8932   55   94   4   892683274           test. head()y_true = test. rating    Creating the model&#182;:       import kerasn_latent_factors_user = 8n_latent_factors_movie = 10n_latent_factors_mf = 3n_users, n_movies = len(dataset. user_id. unique()), len(dataset. item_id. unique())movie_input = keras. layers. Input(shape=[1],name=&#39;Item&#39;)movie_embedding_mlp = keras. layers. Embedding(n_movies + 1, n_latent_factors_movie, name=&#39;Movie-Embedding-MLP&#39;)(movie_input)movie_vec_mlp = keras. layers. Flatten(name=&#39;FlattenMovies-MLP&#39;)(movie_embedding_mlp)movie_vec_mlp = keras. layers. Dropout(0. 2)(movie_vec_mlp)movie_embedding_mf = keras. layers. Embedding(n_movies + 1, n_latent_factors_mf, name=&#39;Movie-Embedding-MF&#39;)(movie_input)movie_vec_mf = keras. layers. Flatten(name=&#39;FlattenMovies-MF&#39;)(movie_embedding_mf)movie_vec_mf = keras. layers. Dropout(0. 2)(movie_vec_mf)user_input = keras. layers. Input(shape=[1],name=&#39;User&#39;)user_vec_mlp = keras. layers. Flatten(name=&#39;FlattenUsers-MLP&#39;)(keras. layers. Embedding(n_users + 1, n_latent_factors_user,name=&#39;User-Embedding-MLP&#39;)(user_input))user_vec_mlp = keras. layers. Dropout(0. 2)(user_vec_mlp)user_vec_mf = keras. layers. Flatten(name=&#39;FlattenUsers-MF&#39;)(keras. layers. Embedding(n_users + 1, n_latent_factors_mf,name=&#39;User-Embedding-MF&#39;)(user_input))user_vec_mf = keras. layers. Dropout(0. 2)(user_vec_mf)concat = keras. layers. merge([movie_vec_mlp, user_vec_mlp], mode=&#39;concat&#39;,name=&#39;Concat&#39;)concat_dropout = keras. layers. Dropout(0. 2)(concat)dense = keras. layers. Dense(200,name=&#39;FullyConnected&#39;)(concat_dropout)dense_batch = keras. layers. BatchNormalization(name=&#39;Batch&#39;)(dense)dropout_1 = keras. layers. Dropout(0. 2,name=&#39;Dropout-1&#39;)(dense_batch)dense_2 = keras. layers. Dense(100,name=&#39;FullyConnected-1&#39;)(dropout_1)dense_batch_2 = keras. layers. BatchNormalization(name=&#39;Batch-2&#39;)(dense_2)dropout_2 = keras. layers. Dropout(0. 2,name=&#39;Dropout-2&#39;)(dense_batch_2)dense_3 = keras. layers. Dense(50,name=&#39;FullyConnected-2&#39;)(dropout_2)dense_4 = keras. layers. Dense(20,name=&#39;FullyConnected-3&#39;, activation=&#39;relu&#39;)(dense_3)pred_mf = keras. layers. merge([movie_vec_mf, user_vec_mf], mode=&#39;dot&#39;,name=&#39;Dot&#39;)pred_mlp = keras. layers. Dense(1, activation=&#39;relu&#39;,name=&#39;Activation&#39;)(dense_4)combine_mlp_mf = keras. layers. merge([pred_mf, pred_mlp], mode=&#39;concat&#39;,name=&#39;Concat-MF-MLP&#39;)result_combine = keras. layers. Dense(100,name=&#39;Combine-MF-MLP&#39;)(combine_mlp_mf)deep_combine = keras. layers. Dense(100,name=&#39;FullyConnected-4&#39;)(result_combine)result = keras. layers. Dense(1,name=&#39;Prediction&#39;)(deep_combine)model = keras. Model([user_input, movie_input], result)opt = keras. optimizers. Adam(lr =0. 01)model. compile(optimizer=&#39;adam&#39;,loss= &#39;mean_absolute_error&#39;)  Using TensorFlow backend.   Let's now see how our model looks like:       from IPython. display import SVGfrom keras. utils. vis_utils import model_to_dotSVG(model_to_dot(model, show_shapes=False, show_layer_names=True, rankdir=&#39;HB&#39;). create(prog=&#39;dot&#39;, format=&#39;svg&#39;))  G4515113056Item: InputLayer4515113392Movie-Embedding-MLP: Embedding4515113056-&gt;4515113392112084624776Movie-Embedding-MF: Embedding4515113056-&gt;1120846247764408641744User: InputLayer112085071184User-Embedding-MLP: Embedding4408641744-&gt;112085071184112085982960User-Embedding-MF: Embedding4408641744-&gt;112085982960112084623992FlattenMovies-MLP: Flatten4515113392-&gt;1120846239924378375728FlattenUsers-MLP: Flatten112085071184-&gt;43783757284515113224dropout_1: Dropout112084623992-&gt;4515113224112085499184dropout_3: Dropout4378375728-&gt;112085499184112085917424Concat: Merge4515113224-&gt;112085917424112085499184-&gt;112085917424112085764920dropout_5: Dropout112085917424-&gt;112085764920112086436832FullyConnected: Dense112085764920-&gt;112086436832112086434816Batch: BatchNormalization112086436832-&gt;112086434816112086597360Dropout-1: Dropout112086434816-&gt;112086597360112086994000FullyConnected-1: Dense112086597360-&gt;112086994000112086761144Batch-2: BatchNormalization112086994000-&gt;112086761144112087744464Dropout-2: Dropout112086761144-&gt;1120877444644399310888FlattenMovies-MF: Flatten112084624776-&gt;43993108884407942728FlattenUsers-MF: Flatten112085982960-&gt;4407942728112087744128FullyConnected-2: Dense112087744464-&gt;112087744128112084624160dropout_2: Dropout4399310888-&gt;112084624160112085423664dropout_4: Dropout4407942728-&gt;112085423664112087225176FullyConnected-3: Dense112087744128-&gt;112087225176112088890336Dot: Merge112084624160-&gt;112088890336112085423664-&gt;112088890336112089386176Activation: Dense112087225176-&gt;112089386176112089474160Concat-MF-MLP: Merge112088890336-&gt;112089474160112089386176-&gt;112089474160112089888696Combine-MF-MLP: Dense112089474160-&gt;112089888696112089888752FullyConnected-4: Dense112089888696-&gt;112089888752112089753920Prediction: Dense112089888752-&gt;112089753920  So, it wasn't very complicated to set up. Courtesy Keras, we can do even more complex stuff!       model. summary()  __________________________________________________________________________________________________Layer (type)          Output Shape     Param #   Connected to           ==================================================================================================Item (InputLayer)        (None, 1)      0                      __________________________________________________________________________________________________User (InputLayer)        (None, 1)      0                      __________________________________________________________________________________________________Movie-Embedding-MLP (Embedding) (None, 1, 10)    16830    Item[0][0]            __________________________________________________________________________________________________User-Embedding-MLP (Embedding) (None, 1, 8)     7552    User[0][0]            __________________________________________________________________________________________________FlattenMovies-MLP (Flatten)   (None, 10)      0      Movie-Embedding-MLP[0][0]    __________________________________________________________________________________________________FlattenUsers-MLP (Flatten)   (None, 8)      0      User-Embedding-MLP[0][0]     __________________________________________________________________________________________________dropout_1 (Dropout)       (None, 10)      0      FlattenMovies-MLP[0][0]     __________________________________________________________________________________________________dropout_3 (Dropout)       (None, 8)      0      FlattenUsers-MLP[0][0]      __________________________________________________________________________________________________Concat (Merge)         (None, 18)      0      dropout_1[0][0]                                          dropout_3[0][0]         __________________________________________________________________________________________________dropout_5 (Dropout)       (None, 18)      0      Concat[0][0]           __________________________________________________________________________________________________FullyConnected (Dense)     (None, 200)     3800    dropout_5[0][0]         __________________________________________________________________________________________________Batch (BatchNormalization)   (None, 200)     800     FullyConnected[0][0]       __________________________________________________________________________________________________Dropout-1 (Dropout)       (None, 200)     0      Batch[0][0]           __________________________________________________________________________________________________FullyConnected-1 (Dense)    (None, 100)     20100    Dropout-1[0][0]         __________________________________________________________________________________________________Batch-2 (BatchNormalization)  (None, 100)     400     FullyConnected-1[0][0]      __________________________________________________________________________________________________Movie-Embedding-MF (Embedding) (None, 1, 3)     5049    Item[0][0]            __________________________________________________________________________________________________User-Embedding-MF (Embedding)  (None, 1, 3)     2832    User[0][0]            __________________________________________________________________________________________________Dropout-2 (Dropout)       (None, 100)     0      Batch-2[0][0]          __________________________________________________________________________________________________FlattenMovies-MF (Flatten)   (None, 3)      0      Movie-Embedding-MF[0][0]     __________________________________________________________________________________________________FlattenUsers-MF (Flatten)    (None, 3)      0      User-Embedding-MF[0][0]     __________________________________________________________________________________________________FullyConnected-2 (Dense)    (None, 50)      5050    Dropout-2[0][0]         __________________________________________________________________________________________________dropout_2 (Dropout)       (None, 3)      0      FlattenMovies-MF[0][0]      __________________________________________________________________________________________________dropout_4 (Dropout)       (None, 3)      0      FlattenUsers-MF[0][0]      __________________________________________________________________________________________________FullyConnected-3 (Dense)    (None, 20)      1020    FullyConnected-2[0][0]      __________________________________________________________________________________________________Dot (Merge)           (None, 1)      0      dropout_2[0][0]                                          dropout_4[0][0]         __________________________________________________________________________________________________Activation (Dense)       (None, 1)      21     FullyConnected-3[0][0]      __________________________________________________________________________________________________Concat-MF-MLP (Merge)      (None, 2)      0      Dot[0][0]                                             Activation[0][0]         __________________________________________________________________________________________________Combine-MF-MLP (Dense)     (None, 100)     300     Concat-MF-MLP[0][0]       __________________________________________________________________________________________________FullyConnected-4 (Dense)    (None, 100)     10100    Combine-MF-MLP[0][0]       __________________________________________________________________________________________________Prediction (Dense)       (None, 1)      101     FullyConnected-4[0][0]      ==================================================================================================Total params: 73,955Trainable params: 73,355Non-trainable params: 600__________________________________________________________________________________________________  We can see that the number of parameters is more than what we had in the Matrix Factorisation case. Let's see how this model works. I'll run it for more epochs given that we have more parameters.       history = model. fit([train. user_id, train. item_id], train. rating, epochs=25, verbose=0, validation_split=0. 1)    Prediction performance of Neural Network based recommender system&#182;:       from sklearn. metrics import mean_absolute_errory_hat_2 = np. round(model. predict([test. user_id, test. item_id]),0)print(mean_absolute_error(y_true, y_hat_2))print(mean_absolute_error(y_true, model. predict([test. user_id, test. item_id])))  0. 7160. 737380115688  Pretty similar to the result we got using matrix factorisation. This isn't very optimised, and I am sure doing so, we can make this approach perform much better than GMF! Thanks for reading. This post has been a good learning experience for me. Hope you enjoyed too! "
    }, {
    "id": 14,
    "url": "https://nipunbatra.github.io/ml/2017/12/18/recommend-keras.html",
    "title": "Recommender Systems in Keras",
    "body": "2017/12/18 -           I have written a few posts earlier about matrix factorisation using various Python libraries. The main application I had in mind for matrix factorisation was recommender systems. In this post, I'll write about using Keras for creating recommender systems. Various people have written excellent similar posts and code that I draw a lot of inspiration from, and give them their credit! I'm assuming that a reader has some experience with Keras, as this post is not intended to be an introduction to Keras. Specifically, in this post, I'll talk about: Matrix Factorisation in KerasAdding non-negativitiy constraints to solve non-negative matrix factorisation (NNMF)Using neural networks for recommendationsI'll be using the Movielens-100k dataset for illustration. There are 943 users and 1682 movies. In total there are a 100k ratings in the dataset. It should be noted that the max. total number of rating for the &lt;users, movies&gt; would be 943*1682, which means that we have about 7% of the total ratings! All rating are on a scale of 1-5. Task&#182;: Given this set of ratings, can we recommend the next set of movies to a user? This would translate to: for every user, estimating the ratings for all the movies that (s)he hasn't watched and maybe recommend the top-k movies by the esimtated ratings! Peak into the dataset&#182;:       import pandas as pdimport numpy as npimport matplotlib. pyplot as pltimport warningswarnings. filterwarnings(&#39;ignore&#39;)%matplotlib inline          dataset = pd. read_csv(&quot;/Users/nipun/Downloads/ml-100k/u. data&quot;,sep=&#39;\t&#39;,names=&quot;user_id,item_id,rating,timestamp&quot;. split(&quot;,&quot;))          dataset. head()           user_id   item_id   rating   timestamp         0   196   242   3   881250949       1   186   302   3   891717742       2   22   377   1   878887116       3   244   51   2   880606923       4   166   346   1   886397596     So, each record (row) shows the rating for a user, item (movie) pair. It should be noted that I use item and movie interchangeably in this post.       len(dataset. user_id. unique()), len(dataset. item_id. unique())  (943, 1682)  We assign a unique number between (0, #users) to each user and do the same for movies.       dataset. user_id = dataset. user_id. astype(&#39;category&#39;). cat. codes. valuesdataset. item_id = dataset. item_id. astype(&#39;category&#39;). cat. codes. values          dataset. head()           user_id   item_id   rating   timestamp         0   195   241   3   881250949       1   185   301   3   891717742       2   21   376   1   878887116       3   243   50   2   880606923       4   165   345   1   886397596     Train test split&#182;: We'll now split our dataset of 100k ratings into train (containing 80k ratings) and test (containing 20k ratings). Given the train set, we'd like to accurately estimate the ratings in the test set.       from sklearn. model_selection import train_test_splittrain, test = train_test_split(dataset, test_size=0. 2)          train. head()           user_id   item_id   rating   timestamp         90092   832   12   2   875036139       50879   94   132   3   888954341       67994   436   12   4   880141129       49769   710   344   4   884485683       11032   121   736   4   879270874           test. head()           user_id   item_id   rating   timestamp         89284   907   493   3   879723046       60499   550   25   4   892785056       11090   373   222   5   880394520       36096   199   140   4   884129346       21633   71   317   5   880037702     Matrix factorisation&#182;: One popular recommender systems approach is called Matrix Factorisation. It works on the principle that we can learn a low-dimensional representation (embedding) of user and movie. For example, for each movie, we can have how much action it has, how long it is, and so on. For each user, we can encode how much they like action, or how much they like long movies, etc. Thus, we can combine the user and the movie embeddings to estimate the ratings on unseen movies. This approach can also be viewed as: given a matrix (A [M X N]) containing users and movies, we want to estimate low dimensional matrices (W [M X k] and H [M X k]), such that: $A \approx W. H^T$ Matrix factorisation in Keras&#182;: We'll now write some code to solve the recommendation problem by matrix factorisation in Keras. We're trying to learn two low-dimensional embeddings of users and items.       import kerasfrom IPython. display import SVGfrom keras. optimizers import Adamfrom keras. utils. vis_utils import model_to_dotn_users, n_movies = len(dataset. user_id. unique()), len(dataset. item_id. unique())n_latent_factors = 3  Using TensorFlow backend.   The key thing is to learn an embedding for movies and users, and then combine them using the dot product! For estimating the rating, for each user, movie pair of interest, we'd take the dot product of the respective user and item embedding. As an example, if we have 2 dimensions in our user and item embedding, which say correspond to [how much user likes action, how much user likes long movies], and the item embedding is [how much action is in the movie, how long is the movie]. Then, we can predict for a user u, and movie m as how much u likes action $\times$ how much action is there in m $+$ how much u likes long movies $\times$ how long is m. Our model would optimise the emebedding such that we minimise the mean squared error on the ratings from the train set.       movie_input = keras. layers. Input(shape=[1],name=&#39;Item&#39;)movie_embedding = keras. layers. Embedding(n_movies + 1, n_latent_factors, name=&#39;Movie-Embedding&#39;)(movie_input)movie_vec = keras. layers. Flatten(name=&#39;FlattenMovies&#39;)(movie_embedding)user_input = keras. layers. Input(shape=[1],name=&#39;User&#39;)user_vec = keras. layers. Flatten(name=&#39;FlattenUsers&#39;)(keras. layers. Embedding(n_users + 1, n_latent_factors,name=&#39;User-Embedding&#39;)(user_input))prod = keras. layers. merge([movie_vec, user_vec], mode=&#39;dot&#39;,name=&#39;DotProduct&#39;)model = keras. Model([user_input, movie_input], prod)model. compile(&#39;adam&#39;, &#39;mean_squared_error&#39;)    Here's a visualisation of our model for a better understanding.       SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir=&#39;HB&#39;). create(prog=&#39;dot&#39;, format=&#39;svg&#39;))  G4651743104Item: InputLayerinput:output:(None, 1)(None, 1)4651743216Movie-Embedding: Embeddinginput:output:(None, 1)(None, 1, 3)4651743104-&gt;46517432164651744392User: InputLayerinput:output:(None, 1)(None, 1)4651743888User-Embedding: Embeddinginput:output:(None, 1)(None, 1, 3)4651744392-&gt;46517438884651744000FlattenMovies: Flatteninput:output:(None, 1, 3)(None, 3)4651743216-&gt;46517440004468062472FlattenUsers: Flatteninput:output:(None, 1, 3)(None, 3)4651743888-&gt;44680624724651881696DotProduct: Mergeinput:output:[(None, 3), (None, 3)](None, 1)4651744000-&gt;46518816964468062472-&gt;4651881696  We can see that in the Merge layer, we take the dot product of the user and the item embeddings to obtain the rating. We can also summarise our model as follows:       model. summary()  __________________________________________________________________________________________________Layer (type)          Output Shape     Param #   Connected to           ==================================================================================================Item (InputLayer)        (None, 1)      0                      __________________________________________________________________________________________________User (InputLayer)        (None, 1)      0                      __________________________________________________________________________________________________Movie-Embedding (Embedding)   (None, 1, 3)     5049    Item[0][0]            __________________________________________________________________________________________________User-Embedding (Embedding)   (None, 1, 3)     2832    User[0][0]            __________________________________________________________________________________________________FlattenMovies (Flatten)     (None, 3)      0      Movie-Embedding[0][0]      __________________________________________________________________________________________________FlattenUsers (Flatten)     (None, 3)      0      User-Embedding[0][0]       __________________________________________________________________________________________________DotProduct (Merge)       (None, 1)      0      FlattenMovies[0][0]                                        FlattenUsers[0][0]        ==================================================================================================Total params: 7,881Trainable params: 7,881Non-trainable params: 0__________________________________________________________________________________________________  So, we have 7881 parameters to learn! Let's train our model now!       history = model. fit([train. user_id, train. item_id], train. rating, epochs=100, verbose=0)    Train error v/s epoch number&#182;: Before we test how well our model does in the test setting, we can visualise the train loss with epoch number.       pd. Series(history. history[&#39;loss&#39;]). plot(logy=True)plt. xlabel(&quot;Epoch&quot;)plt. ylabel(&quot;Train Error&quot;)  &lt;matplotlib. text. Text at 0x1155a07b8&gt;  Prediction error&#182;: Let's now see how our model does! I'll do a small post-processing step to round off our prediction to the nearest integer. This is usually not done, and thus just a whimsical step, since the training ratings are all integers! There are better ways to encode this intger requirement (one-hot encoding!), but we won't discuss them in this post.       y_hat = np. round(model. predict([test. user_id, test. item_id]),0)y_true = test. rating          from sklearn. metrics import mean_absolute_errormean_absolute_error(y_true, y_hat)  0. 6915  Not bad! We're able to get a $MAE$ of 0. 69! I'm sure with a bit of parameter/hyper-parameter optimisation, we may be able to improve the results. However, I won't talk about these optimisations in this post. Extracting the learnt embeddings&#182;: We can extract the learnt movie and item embeddings as follows:       movie_embedding_learnt = model. get_layer(name=&#39;Movie-Embedding&#39;). get_weights()[0]pd. DataFrame(movie_embedding_learnt). describe()           0   1   2         count   1683. 000000   1683. 000000   1683. 000000       mean   -0. 935420   0. 857862   0. 954169       std   0. 517458   0. 447439   0. 458095       min   -2. 524487   -0. 459752   -0. 989537       25%   -1. 323431   0. 546364   0. 642444       50%   -0. 949188   0. 851243   0. 993619       75%   -0. 550862   1. 159588   1. 283555       max   0. 500618   2. 140607   2. 683658           user_embedding_learnt = model. get_layer(name=&#39;User-Embedding&#39;). get_weights()[0]pd. DataFrame(user_embedding_learnt). describe()           0   1   2         count   944. 000000   944. 000000   944. 000000       mean   -1. 126231   1. 171609   1. 109131       std   0. 517478   0. 409016   0. 548384       min   -2. 883226   -0. 500010   -0. 415373       25%   -1. 458197   0. 903574   0. 735729       50%   -1. 159480   1. 199517   1. 084089       75%   -0. 836746   1. 456610   1. 468611       max   0. 899436   2. 605330   2. 826109     We can see that both the user and the item embeddings have negative elements. There are some applications which require that the learnt embeddings be non-negative. This approach is also called non-negative matrix factorisation, which we'll workout now. Non-negative Matrix factorisation (NNMF) in Keras&#182;: The code for NNMF remains exactly the same as the code for matrix factorisation. The only change is that we add non-negativity constraints on the learnt embeddings. This is done as follows:       from keras. constraints import non_negmovie_input = keras. layers. Input(shape=[1],name=&#39;Item&#39;)movie_embedding = keras. layers. Embedding(n_movies + 1, n_latent_factors, name=&#39;NonNegMovie-Embedding&#39;, embeddings_constraint=non_neg())(movie_input)movie_vec = keras. layers. Flatten(name=&#39;FlattenMovies&#39;)(movie_embedding)user_input = keras. layers. Input(shape=[1],name=&#39;User&#39;)user_vec = keras. layers. Flatten(name=&#39;FlattenUsers&#39;)(keras. layers. Embedding(n_users + 1, n_latent_factors,name=&#39;NonNegUser-Embedding&#39;,embeddings_constraint=non_neg())(user_input))prod = keras. layers. merge([movie_vec, user_vec], mode=&#39;dot&#39;,name=&#39;DotProduct&#39;)model = keras. Model([user_input, movie_input], prod)model. compile(&#39;adam&#39;, &#39;mean_squared_error&#39;)    We now verify if we are indeed able to learn non-negative embeddings. I'll not compare the performance of NNMF on the test set, in the interest of space.       history_nonneg = model. fit([train. user_id, train. item_id], train. rating, epochs=10, verbose=0)          movie_embedding_learnt = model. get_layer(name=&#39;NonNegMovie-Embedding&#39;). get_weights()[0]pd. DataFrame(movie_embedding_learnt). describe()           0   1   2         count   1683. 000000   1683. 000000   1683. 000000       mean   0. 838450   0. 840330   0. 838066       std   0. 301618   0. 301529   0. 301040       min   -0. 000000   -0. 000000   -0. 000000       25%   0. 657749   0. 663951   0. 656453       50%   0. 901495   0. 904192   0. 895934       75%   1. 072706   1. 073591   1. 072926       max   1. 365719   1. 379006   1. 373672     Looks good! Neural networks for recommendation&#182;: We'll now create a simple neural network for recommendation, or for estimating rating! This model is very similar to the earlier matrix factorisation models, but differs in the following ways: Instead of taking a dot product of the user and the item embedding, we concatenate them and use them as features for our neural network. Thus, we are not constrained to the dot product way of combining the embeddings, and can learn complex non-linear relationships. Due to #1, we can now have a different dimension of user and item embeddings. This can be useful if one dimension is larger than the other.       n_latent_factors_user = 5n_latent_factors_movie = 8movie_input = keras. layers. Input(shape=[1],name=&#39;Item&#39;)movie_embedding = keras. layers. Embedding(n_movies + 1, n_latent_factors_movie, name=&#39;Movie-Embedding&#39;)(movie_input)movie_vec = keras. layers. Flatten(name=&#39;FlattenMovies&#39;)(movie_embedding)movie_vec = keras. layers. Dropout(0. 2)(movie_vec)user_input = keras. layers. Input(shape=[1],name=&#39;User&#39;)user_vec = keras. layers. Flatten(name=&#39;FlattenUsers&#39;)(keras. layers. Embedding(n_users + 1, n_latent_factors_user,name=&#39;User-Embedding&#39;)(user_input))user_vec = keras. layers. Dropout(0. 2)(user_vec)concat = keras. layers. merge([movie_vec, user_vec], mode=&#39;concat&#39;,name=&#39;Concat&#39;)concat_dropout = keras. layers. Dropout(0. 2)(concat)dense = keras. layers. Dense(200,name=&#39;FullyConnected&#39;)(concat)dropout_1 = keras. layers. Dropout(0. 2,name=&#39;Dropout&#39;)(dense)dense_2 = keras. layers. Dense(100,name=&#39;FullyConnected-1&#39;)(concat)dropout_2 = keras. layers. Dropout(0. 2,name=&#39;Dropout&#39;)(dense_2)dense_3 = keras. layers. Dense(50,name=&#39;FullyConnected-2&#39;)(dense_2)dropout_3 = keras. layers. Dropout(0. 2,name=&#39;Dropout&#39;)(dense_3)dense_4 = keras. layers. Dense(20,name=&#39;FullyConnected-3&#39;, activation=&#39;relu&#39;)(dense_3)result = keras. layers. Dense(1, activation=&#39;relu&#39;,name=&#39;Activation&#39;)(dense_4)adam = Adam(lr=0. 005)model = keras. Model([user_input, movie_input], result)model. compile(optimizer=adam,loss= &#39;mean_absolute_error&#39;)    Let's now see how our model looks like:       SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir=&#39;HB&#39;). create(prog=&#39;dot&#39;, format=&#39;svg&#39;))  G112307868840Item: InputLayerinput:output:(None, 1)(None, 1)112308383136Movie-Embedding: Embeddinginput:output:(None, 1)(None, 1, 8)112307868840-&gt;1123083831364659651864User: InputLayerinput:output:(None, 1)(None, 1)112310319536User-Embedding: Embeddinginput:output:(None, 1)(None, 1, 5)4659651864-&gt;112310319536112308383416FlattenMovies: Flatteninput:output:(None, 1, 8)(None, 8)112308383136-&gt;112308383416112307982232FlattenUsers: Flatteninput:output:(None, 1, 5)(None, 5)112310319536-&gt;112307982232112308313840dropout_1: Dropoutinput:output:(None, 8)(None, 8)112308383416-&gt;112308313840112310320768dropout_2: Dropoutinput:output:(None, 5)(None, 5)112307982232-&gt;1123103207684659651360Concat: Mergeinput:output:[(None, 8), (None, 5)](None, 13)112308313840-&gt;4659651360112310320768-&gt;4659651360112308749368FullyConnected-1: Denseinput:output:(None, 13)(None, 100)4659651360-&gt;112308749368112310118104FullyConnected-2: Denseinput:output:(None, 100)(None, 50)112308749368-&gt;1123101181044653345424FullyConnected-3: Denseinput:output:(None, 50)(None, 20)112310118104-&gt;46533454244653179904Activation: Denseinput:output:(None, 20)(None, 1)4653345424-&gt;4653179904  It should be noted that we use a different number of embeddings for user (3) and items (5)! These combine to form a vector of length (5+3 = 8), which is then fed into the neural network. We also add a dropout layer to prevent overfitting!       model. summary()  __________________________________________________________________________________________________Layer (type)          Output Shape     Param #   Connected to           ==================================================================================================Item (InputLayer)        (None, 1)      0                      __________________________________________________________________________________________________User (InputLayer)        (None, 1)      0                      __________________________________________________________________________________________________Movie-Embedding (Embedding)   (None, 1, 8)     13464    Item[0][0]            __________________________________________________________________________________________________User-Embedding (Embedding)   (None, 1, 5)     4720    User[0][0]            __________________________________________________________________________________________________FlattenMovies (Flatten)     (None, 8)      0      Movie-Embedding[0][0]      __________________________________________________________________________________________________FlattenUsers (Flatten)     (None, 5)      0      User-Embedding[0][0]       __________________________________________________________________________________________________dropout_1 (Dropout)       (None, 8)      0      FlattenMovies[0][0]       __________________________________________________________________________________________________dropout_2 (Dropout)       (None, 5)      0      FlattenUsers[0][0]        __________________________________________________________________________________________________Concat (Merge)         (None, 13)      0      dropout_1[0][0]                                          dropout_2[0][0]         __________________________________________________________________________________________________FullyConnected-1 (Dense)    (None, 100)     1400    Concat[0][0]           __________________________________________________________________________________________________FullyConnected-2 (Dense)    (None, 50)      5050    FullyConnected-1[0][0]      __________________________________________________________________________________________________FullyConnected-3 (Dense)    (None, 20)      1020    FullyConnected-2[0][0]      __________________________________________________________________________________________________Activation (Dense)       (None, 1)      21     FullyConnected-3[0][0]      ==================================================================================================Total params: 25,675Trainable params: 25,675Non-trainable params: 0__________________________________________________________________________________________________  We can see that the number of parameters is more than what we had in the Matrix Factorisation case. Let's see how this model works. I'll run it for more epochs given that we have more parameters.       history = model. fit([train. user_id, train. item_id], train. rating, epochs=250, verbose=0)    Prediction performance of Neural Network based recommender system&#182;:       y_hat_2 = np. round(model. predict([test. user_id, test. item_id]),0)print(mean_absolute_error(y_true, y_hat_2))print(mean_absolute_error(y_true, model. predict([test. user_id, test. item_id])))  0. 69570. 708807692927  Pretty similar to the result we got using matrix factorisation. Maybe, we need to tweak around a lot more with the neural network to get better results? Thanks for reading. This post has been a good learning experience for me. Hope you enjoyed too! "
    }, {
    "id": 15,
    "url": "https://nipunbatra.github.io/ml/2017/08/13/mf-autograd-adagrad.html",
    "title": "Adagrad based matrix factorization",
    "body": "2017/08/13 -           In a previous post, we had seen how to perfom non-negative matrix factorization (NNMF) using Tensorflow. In another previous post, I had shown how to use Adagrad for linear regression. This current post can be considered an extension of the linear regression using Adagrad post. Just for the purpose of education, I'll poorly initialise the estimate of one of the decomposed matrix, to see how well Adagrad can adjust weights! Customary imports&#182;:       import autograd. numpy as npimport pandas as pdimport matplotlib. pyplot as pltimport seaborn as snsfrom matplotlib. animation import FuncAnimationfrom matplotlib import gridspec%matplotlib inline    Creating the matrix to be decomposed&#182;:       A = np. array([[3, 4, 5, 2],          [4, 4, 3, 3],          [5, 5, 4, 3]], dtype=np. float32). T    Masking one entry&#182;:       A[0, 0] = np. NAN          A  array([[ nan,  4. ,  5. ],    [ 4. ,  4. ,  5. ],    [ 5. ,  3. ,  4. ],    [ 2. ,  3. ,  3. ]], dtype=float32)  Defining the cost function&#182;:       def cost(param_list):  W, H = param_list  pred = np. dot(W, H)  mask = ~np. isnan(A)  return np. sqrt(((pred - A)[mask]. flatten() ** 2). mean(axis=None))    Decomposition params&#182;:       rank = 2learning_rate=0. 01n_steps = 10000    Adagrad routine&#182;:       def adagrad_gd(param_init, cost, niter=5, lr=1e-2, eps=1e-8, random_seed=0):  &quot;&quot;&quot;  param_init: List of initial values of parameters  cost: cost function  niter: Number of iterations to run  lr: Learning rate  eps: Fudge factor, to avoid division by zero  &quot;&quot;&quot;  from copy import deepcopy  from autograd import grad  # Fixing the random_seed  np. random. seed(random_seed)    # Function to compute the gradient of the cost function  grad_cost = grad(cost)  params = deepcopy(param_init)  param_array, grad_array, lr_array, cost_array = [params], [], [[lr*np. ones_like(_) for _ in params]], [cost(params)]  # Initialising sum of squares of gradients for each param as 0  sum_squares_gradients = [np. zeros_like(param) for param in params]  for i in range(niter):    out_params = []    gradients = grad_cost(params)    # At each iteration, we add the square of the gradients to `sum_squares_gradients`    sum_squares_gradients= [eps + sum_prev + np. square(g) for sum_prev, g in zip(sum_squares_gradients, gradients)]    # Adapted learning rate for parameter list    lrs = [np. divide(lr, np. sqrt(sg)) for sg in sum_squares_gradients]    # Paramter update    params = [param-(adapted_lr*grad_param) for param, adapted_lr, grad_param in zip(params, lrs, gradients)]    param_array. append(params)    lr_array. append(lrs)    grad_array. append(gradients)    cost_array. append(cost(params))      return params, param_array, grad_array, lr_array, cost_array    Running Adagrad&#182;: Fixing initial parameters&#182;: I'm poorly initialising H here to see how the learning rates vary for W and H.       np. random. seed(0)shape = A. shapeH_init = -5*np. abs(np. random. randn(rank, shape[1]))W_init = np. abs(np. random. randn(shape[0], rank))param_init = [W_init, H_init]          H_init  array([[ -8. 82026173, -2. 00078604, -4. 89368992],    [-11. 204466 , -9. 33778995, -4. 8863894 ]])        W_init  array([[ 0. 95008842, 0. 15135721],    [ 0. 10321885, 0. 4105985 ],    [ 0. 14404357, 1. 45427351],    [ 0. 76103773, 0. 12167502]])        # Cost for initial set of parameterscost(param_init)  11. 651268820608442        lr = 0. 1eps=1e-8niter=2000ada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init, cost, niter=niter, lr=lr, eps=eps)    Cost v/s # iterations&#182;:       pd. Series(ada_cost_array). plot(logy=True)plt. ylabel(&quot;Cost (log scale)&quot;)plt. xlabel(&quot;# Iterations&quot;)  &lt;matplotlib. text. Text at 0x10ece7610&gt;  Final set of parameters and recovered matrix&#182;:       W_final, H_final = ada_paramspred = np. dot(W_final, H_final)pred_df = pd. DataFrame(pred). round()pred_df           0   1   2         0   5. 0   4. 0   5. 0       1   4. 0   4. 0   5. 0       2   5. 0   3. 0   4. 0       3   2. 0   3. 0   3. 0     Learning rate evolution for W&#182;:       W_lrs = np. array(ada_lr_array)[:, 0]          W_lrs = np. array(ada_lr_array)[:, 0]fig= plt. figure(figsize=(4, 2))gs = gridspec. GridSpec(1, 2, width_ratios=[8, 1]) ax = plt. subplot(gs[0]), plt. subplot(gs[1])max_W, min_W = np. max([np. max(x) for x in W_lrs]), np. min([np. min(x) for x in W_lrs])def update(iteration):  ax[0]. cla()  ax[1]. cla()  sns. heatmap(W_lrs[iteration], vmin=min_W, vmax=max_W, ax=ax[0], annot=True, fmt=&#39;. 4f&#39;, cbar_ax=ax[1])  ax[0]. set_title(&quot;Learning rate update for W\nIteration: {}&quot;. format(iteration))  fig. tight_layout()anim = FuncAnimation(fig, update, frames=np. arange(0, 200, 10), interval=500)anim. save(&#39;W_update. gif&#39;, dpi=80, writer=&#39;imagemagick&#39;)plt. close()     Learning rate evolution for H&#182;:       H_lrs = np. array(ada_lr_array)[:, 1]fig= plt. figure(figsize=(4, 2))gs = gridspec. GridSpec(1, 2, width_ratios=[10, 1]) ax = plt. subplot(gs[0]), plt. subplot(gs[1])max_H, min_H = np. max([np. max(x) for x in H_lrs]), np. min([np. min(x) for x in H_lrs])def update(iteration):  ax[0]. cla()  ax[1]. cla()  sns. heatmap(H_lrs[iteration], vmin=min_H, vmax=max_H, ax=ax[0], annot=True, fmt=&#39;. 2f&#39;, cbar_ax=ax[1])  ax[0]. set_title(&quot;Learning rate update for H\nIteration: {}&quot;. format(iteration))  fig. tight_layout()anim = FuncAnimation(fig, update, frames=np. arange(0, 200, 10), interval=500)anim. save(&#39;H_update. gif&#39;, dpi=80, writer=&#39;imagemagick&#39;)plt. close()     "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}